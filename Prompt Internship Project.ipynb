{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a21be723-6155-40a9-8702-ed71969f7556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\suraj garole\\conda\\lib\\site-packages (4.56.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\suraj garole\\conda\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\suraj garole\\conda\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (0.35.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\suraj garole\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\suraj garole\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentence-transformers PyPDF2 faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad0bfae3-60fb-442a-b0a0-f79a6770b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0513cd35-6c88-401c-99bd-67ffc94781a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\suraj garole\\conda\\lib\\site-packages (4.56.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\suraj garole\\conda\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\suraj garole\\conda\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (0.35.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\suraj garole\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\suraj garole\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "PDF loaded successfully. Total characters: 114\n",
      "Total chunks created: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee0ab5fce614c699b958ebc5f690e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated for all chunks.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings generated for all chunks.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Step 6: Build FAISS index\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m chunk_embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     41\u001b[0m index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatL2(embedding_dim)\n\u001b[0;32m     42\u001b[0m index\u001b[38;5;241m.\u001b[39madd(chunk_embeddings)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Step 1: Install libraries (run once)\n",
    "!pip install transformers sentence-transformers PyPDF2 faiss-cpu\n",
    "\n",
    "# Step 2: Imports\n",
    "import PyPDF2\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Step 3: Load your PDF\n",
    "pdf_path = r\"C:\\Users\\Suraj Garole\\OneDrive\\Documents\\Python Notes.pdf\"  # your file path\n",
    "all_text = \"\"\n",
    "\n",
    "with open(pdf_path, \"rb\") as f:\n",
    "    pdf_reader = PyPDF2.PdfReader(f)\n",
    "    for page in pdf_reader.pages:\n",
    "        all_text += page.extract_text() + \" \"\n",
    " \n",
    "print(\"PDF loaded successfully. Total characters:\", len(all_text))\n",
    "\n",
    "# Step 4: Split text into chunks\n",
    "def split_text(text, chunk_size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text(all_text)\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "\n",
    "# Step 5: Generate embeddings\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "chunk_embeddings = embedder.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
    "print(\"Embeddings generated for all chunks.\")\n",
    "\n",
    "# Step 6: Build FAISS index\n",
    "embedding_dim = chunk_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(chunk_embeddings)\n",
    "print(f\"FAISS index built with {index.ntotal} vectors.\")\n",
    "\n",
    "# Step 7: Load GPT model (GPT-Neo 1.3B)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=-1)  # CPU: -1, GPU: 0\n",
    "\n",
    "# Step 8: Define Q&A function\n",
    "def answer_question(query, k=3):\n",
    "    # Embed the query\n",
    "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # Search in FAISS\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    \n",
    "    # Retrieve top-k chunks\n",
    "    context = \" \".join([chunks[i] for i in I[0]])\n",
    "    \n",
    "    # Generate answer using GPT-Neo\n",
    "    prompt = f\"Answer the question based on the context below:\\n\\nContext: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    result = generator(prompt, max_length=200, do_sample=True, top_p=0.95, top_k=50)\n",
    "    return result[0]['generated_text'].split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# Step 9: Test the Q&A system\n",
    "query = \"What is the purpose of the ESP system?\"\n",
    "answer = answer_question(query)\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32c35cec-2108-40b0-b0ab-c324494169b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\suraj garole\\conda\\lib\\site-packages (4.56.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\suraj garole\\conda\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\suraj garole\\conda\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\suraj garole\\conda\\lib\\site-packages (8.1.7)\n",
      "Requirement already satisfied: filelock in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (0.35.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from ipywidgets) (8.30.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in c:\\users\\suraj garole\\conda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\suraj garole\\conda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\suraj garole\\conda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\suraj garole\\conda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\suraj garole\\conda\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\suraj garole\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\suraj garole\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: executing in c:\\users\\suraj garole\\conda\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\suraj garole\\conda\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\suraj garole\\conda\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "PDF loaded successfully. Total characters: 0\n",
      "Total chunks created: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c0507e029944498369d7ab866e41b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated for all chunks.\n",
      "FAISS index built with 1 vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87cd7950b6c4f3d80b07e6827ad3c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Question:', layout=Layout(width='80%'), placeholder='Type your question here...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ae24b961224665bfbbd5fb03f84e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Ask AI', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32f136f5ad04c3e8c0f3152dbc94e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===============================\n",
    "# Step 0: Install required libraries\n",
    "# ===============================\n",
    "!pip install transformers sentence-transformers PyPDF2 faiss-cpu ipywidgets\n",
    "\n",
    "# ===============================\n",
    "# Step 1: Imports\n",
    "# ===============================\n",
    "import PyPDF2\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# ===============================\n",
    "# Step 2: Load your PDF\n",
    "# ===============================\n",
    "pdf_path = r\"C:\\Users\\Suraj Garole\\OneDrive\\Documents\\Python Notes.pdf\"  \n",
    "all_text = \"\"\n",
    "\n",
    "with open(pdf_path, \"rb\") as f:\n",
    "    pdf_reader = PyPDF2.PdfReader(f)\n",
    "    for page in pdf_reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            all_text += page_text + \" \"\n",
    "\n",
    "print(\"PDF loaded successfully. Total characters:\", len(all_text))\n",
    "\n",
    "# ===============================\n",
    "# Step 3: Split text into chunks\n",
    "# ===============================\n",
    "def split_text(text, chunk_size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text(all_text)\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "\n",
    "# ===============================\n",
    "# Step 4: Generate embeddings\n",
    "# ===============================\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "chunk_embeddings = embedder.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# Ensure embeddings are 2D (FAISS requirement)\n",
    "if len(chunk_embeddings.shape) == 1:\n",
    "    chunk_embeddings = np.expand_dims(chunk_embeddings, axis=0)\n",
    "\n",
    "print(\"Embeddings generated for all chunks.\")\n",
    "\n",
    "# ===============================\n",
    "# Step 5: Build FAISS index\n",
    "# ===============================\n",
    "embedding_dim = chunk_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(chunk_embeddings)\n",
    "print(f\"FAISS index built with {index.ntotal} vectors.\")\n",
    "\n",
    "# ===============================\n",
    "# Step 6: Load GPT-Neo for answer generation\n",
    "# ===============================\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=-1)  # CPU=-1, GPU=0\n",
    "\n",
    "# ===============================\n",
    "# Step 7: Define Q&A function\n",
    "# ===============================\n",
    "def answer_question(query, k=3):\n",
    "    # Embed the query\n",
    "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # FAISS search\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    \n",
    "    # Retrieve top-k chunks\n",
    "    context = \" \".join([chunks[i] for i in I[0]])\n",
    "    \n",
    "    # GPT-Neo prompt\n",
    "    prompt = f\"Answer the question based on the context below:\\n\\nContext: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    result = generator(prompt, max_length=200, do_sample=True, top_p=0.95, top_k=50)\n",
    "    return result[0]['generated_text'].split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# ===============================\n",
    "# Step 8: Interactive Q&A in Jupyter\n",
    "# ===============================\n",
    "# Textbox for question\n",
    "question_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type your question here...',\n",
    "    description='Question:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "# Button to submit question\n",
    "submit_button = widgets.Button(description=\"Ask AI\")\n",
    "\n",
    "# Output area\n",
    "output_area = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "# Function to handle button click\n",
    "def on_submit_button_clicked(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        query = question_box.value\n",
    "        if query.strip() == \"\":\n",
    "            print(\"Please type a question!\")\n",
    "            return\n",
    "        print(\"Question:\", query)\n",
    "        answer = answer_question(query)\n",
    "        print(\"Answer:\", answer)\n",
    "\n",
    "submit_button.on_click(on_submit_button_clicked)\n",
    "\n",
    "# Display interface\n",
    "display(question_box, submit_button, output_area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88a3dee7-4c48-4160-a34f-f450f3fd37bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save chunks\n",
    "with open(\"chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)\n",
    "\n",
    "# Save embeddings\n",
    "np.save(\"chunk_embeddings.npy\", chunk_embeddings)\n",
    "\n",
    "print(\"Knowledge base saved successfully!\")\n",
    "\n",
    "# --- To load later ---\n",
    "# with open(\"chunks.pkl\", \"rb\") as f:\n",
    "#     chunks = pickle.load(f)\n",
    "# chunk_embeddings = np.load(\"chunk_embeddings.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a32fa1c9-d62b-47a6-aaf9-074dd167f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "def answer_question_with_history(query, k=3):\n",
    "    answer = answer_question(query, k)\n",
    "    chat_history.append({\"Question\": query, \"Answer\": answer})\n",
    "    return answer\n",
    "\n",
    "# Optional: display session history\n",
    "def show_chat_history():\n",
    "    for entry in chat_history:\n",
    "        print(\"Q:\", entry[\"Question\"])\n",
    "        print(\"A:\", entry[\"Answer\"])\n",
    "        print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b380dce3-ac98-4639-9f0d-d164b8492dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(query, k=3):\n",
    "    # Embed the query\n",
    "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # Ensure query_embedding is 2D\n",
    "    if len(query_embedding.shape) == 1:\n",
    "        query_embedding = np.expand_dims(query_embedding, axis=0)\n",
    "    \n",
    "    # FAISS search\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    \n",
    "    # Retrieve top-k chunks\n",
    "    context = \" \".join([chunks[i] for i in I[0]])\n",
    "    \n",
    "    # GPT-Neo prompt\n",
    "    prompt = f\"Answer the question based on the context below:\\n\\nContext: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    result = generator(prompt, max_length=200, do_sample=True, top_p=0.95, top_k=50)\n",
    "    return result[0]['generated_text'].split(\"Answer:\")[-1].strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abb10652-8a55-4651-afaf-20b1cac4fa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SpeechRecognition\n",
      "  Downloading speechrecognition-3.14.3-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting pyttsx3\n",
      "  Downloading pyttsx3-2.99-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pyaudio\n",
      "  Downloading PyAudio-0.2.14-cp313-cp313-win_amd64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\suraj garole\\conda\\lib\\site-packages (from SpeechRecognition) (4.12.2)\n",
      "Collecting standard-aifc (from SpeechRecognition)\n",
      "  Downloading standard_aifc-3.13.0-py3-none-any.whl.metadata (969 bytes)\n",
      "Requirement already satisfied: audioop-lts in c:\\users\\suraj garole\\conda\\lib\\site-packages (from SpeechRecognition) (0.2.2)\n",
      "Collecting comtypes (from pyttsx3)\n",
      "  Downloading comtypes-1.4.12-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting pypiwin32 (from pyttsx3)\n",
      "  Downloading pypiwin32-223-py3-none-any.whl.metadata (236 bytes)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pyttsx3) (308)\n",
      "Collecting standard-chunk (from standard-aifc->SpeechRecognition)\n",
      "  Downloading standard_chunk-3.13.0-py3-none-any.whl.metadata (860 bytes)\n",
      "Downloading speechrecognition-3.14.3-py3-none-any.whl (32.9 MB)\n",
      "   ---------------------------------------- 0.0/32.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/32.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/32.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/32.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/32.9 MB 458.6 kB/s eta 0:01:11\n",
      "    --------------------------------------- 0.5/32.9 MB 458.6 kB/s eta 0:01:11\n",
      "    --------------------------------------- 0.8/32.9 MB 516.3 kB/s eta 0:01:03\n",
      "    --------------------------------------- 0.8/32.9 MB 516.3 kB/s eta 0:01:03\n",
      "   - -------------------------------------- 1.0/32.9 MB 557.8 kB/s eta 0:00:58\n",
      "   - -------------------------------------- 1.3/32.9 MB 599.1 kB/s eta 0:00:53\n",
      "   - -------------------------------------- 1.3/32.9 MB 599.1 kB/s eta 0:00:53\n",
      "   -- ------------------------------------- 1.8/32.9 MB 585.2 kB/s eta 0:00:54\n",
      "   -- ------------------------------------- 2.1/32.9 MB 579.2 kB/s eta 0:00:54\n",
      "   -- ------------------------------------- 2.1/32.9 MB 579.2 kB/s eta 0:00:54\n",
      "   -- ------------------------------------- 2.4/32.9 MB 582.5 kB/s eta 0:00:53\n",
      "   --- ------------------------------------ 2.6/32.9 MB 594.6 kB/s eta 0:00:51\n",
      "   --- ------------------------------------ 2.6/32.9 MB 594.6 kB/s eta 0:00:51\n",
      "   --- ------------------------------------ 2.9/32.9 MB 584.1 kB/s eta 0:00:52\n",
      "   --- ------------------------------------ 3.1/32.9 MB 584.9 kB/s eta 0:00:51\n",
      "   ---- ----------------------------------- 3.4/32.9 MB 585.6 kB/s eta 0:00:51\n",
      "   ---- ----------------------------------- 3.4/32.9 MB 585.6 kB/s eta 0:00:51\n",
      "   ---- ----------------------------------- 3.7/32.9 MB 583.8 kB/s eta 0:00:50\n",
      "   ---- ----------------------------------- 3.7/32.9 MB 583.8 kB/s eta 0:00:50\n",
      "   ---- ----------------------------------- 3.9/32.9 MB 588.7 kB/s eta 0:00:50\n",
      "   ---- ----------------------------------- 3.9/32.9 MB 588.7 kB/s eta 0:00:50\n",
      "   ----- ---------------------------------- 4.2/32.9 MB 587.7 kB/s eta 0:00:49\n",
      "   ----- ---------------------------------- 4.5/32.9 MB 591.6 kB/s eta 0:00:48\n",
      "   ----- ---------------------------------- 4.5/32.9 MB 591.6 kB/s eta 0:00:48\n",
      "   ----- ---------------------------------- 4.7/32.9 MB 601.9 kB/s eta 0:00:47\n",
      "   ------ --------------------------------- 5.0/32.9 MB 610.6 kB/s eta 0:00:46\n",
      "   ------ --------------------------------- 5.2/32.9 MB 623.0 kB/s eta 0:00:45\n",
      "   ------ --------------------------------- 5.5/32.9 MB 629.4 kB/s eta 0:00:44\n",
      "   ------ --------------------------------- 5.5/32.9 MB 629.4 kB/s eta 0:00:44\n",
      "   ------- -------------------------------- 5.8/32.9 MB 634.0 kB/s eta 0:00:43\n",
      "   ------- -------------------------------- 6.0/32.9 MB 642.8 kB/s eta 0:00:42\n",
      "   ------- -------------------------------- 6.3/32.9 MB 654.4 kB/s eta 0:00:41\n",
      "   ------- -------------------------------- 6.6/32.9 MB 663.0 kB/s eta 0:00:40\n",
      "   ------- -------------------------------- 6.6/32.9 MB 663.0 kB/s eta 0:00:40\n",
      "   -------- ------------------------------- 6.8/32.9 MB 663.1 kB/s eta 0:00:40\n",
      "   -------- ------------------------------- 7.1/32.9 MB 673.9 kB/s eta 0:00:39\n",
      "   -------- ------------------------------- 7.3/32.9 MB 683.5 kB/s eta 0:00:38\n",
      "   --------- ------------------------------ 7.6/32.9 MB 692.2 kB/s eta 0:00:37\n",
      "   --------- ------------------------------ 7.6/32.9 MB 692.2 kB/s eta 0:00:37\n",
      "   --------- ------------------------------ 7.9/32.9 MB 683.1 kB/s eta 0:00:37\n",
      "   --------- ------------------------------ 8.1/32.9 MB 691.2 kB/s eta 0:00:36\n",
      "   --------- ------------------------------ 8.1/32.9 MB 691.2 kB/s eta 0:00:36\n",
      "   ---------- ----------------------------- 8.4/32.9 MB 687.9 kB/s eta 0:00:36\n",
      "   ---------- ----------------------------- 8.4/32.9 MB 687.9 kB/s eta 0:00:36\n",
      "   ---------- ----------------------------- 8.7/32.9 MB 687.2 kB/s eta 0:00:36\n",
      "   ---------- ----------------------------- 8.9/32.9 MB 687.2 kB/s eta 0:00:35\n",
      "   ---------- ----------------------------- 8.9/32.9 MB 687.2 kB/s eta 0:00:35\n",
      "   ----------- ---------------------------- 9.4/32.9 MB 691.3 kB/s eta 0:00:34\n",
      "   ----------- ---------------------------- 9.4/32.9 MB 691.3 kB/s eta 0:00:34\n",
      "   ----------- ---------------------------- 9.7/32.9 MB 695.9 kB/s eta 0:00:34\n",
      "   ------------ --------------------------- 10.0/32.9 MB 695.0 kB/s eta 0:00:33\n",
      "   ------------ --------------------------- 10.0/32.9 MB 695.0 kB/s eta 0:00:33\n",
      "   ------------ --------------------------- 10.2/32.9 MB 691.2 kB/s eta 0:00:33\n",
      "   ------------ --------------------------- 10.5/32.9 MB 691.2 kB/s eta 0:00:33\n",
      "   ------------ --------------------------- 10.5/32.9 MB 691.2 kB/s eta 0:00:33\n",
      "   ------------- -------------------------- 10.7/32.9 MB 685.5 kB/s eta 0:00:33\n",
      "   ------------- -------------------------- 11.0/32.9 MB 681.9 kB/s eta 0:00:33\n",
      "   ------------- -------------------------- 11.0/32.9 MB 681.9 kB/s eta 0:00:33\n",
      "   ------------- -------------------------- 11.3/32.9 MB 681.7 kB/s eta 0:00:32\n",
      "   -------------- ------------------------- 11.5/32.9 MB 682.0 kB/s eta 0:00:32\n",
      "   -------------- ------------------------- 11.5/32.9 MB 682.0 kB/s eta 0:00:32\n",
      "   -------------- ------------------------- 11.8/32.9 MB 683.3 kB/s eta 0:00:31\n",
      "   -------------- ------------------------- 12.1/32.9 MB 686.2 kB/s eta 0:00:31\n",
      "   --------------- ------------------------ 12.3/32.9 MB 688.1 kB/s eta 0:00:30\n",
      "   --------------- ------------------------ 12.6/32.9 MB 685.4 kB/s eta 0:00:30\n",
      "   --------------- ------------------------ 12.6/32.9 MB 685.4 kB/s eta 0:00:30\n",
      "   --------------- ------------------------ 12.8/32.9 MB 680.7 kB/s eta 0:00:30\n",
      "   --------------- ------------------------ 12.8/32.9 MB 680.7 kB/s eta 0:00:30\n",
      "   --------------- ------------------------ 13.1/32.9 MB 681.7 kB/s eta 0:00:29\n",
      "   ---------------- ----------------------- 13.4/32.9 MB 683.3 kB/s eta 0:00:29\n",
      "   ---------------- ----------------------- 13.4/32.9 MB 683.3 kB/s eta 0:00:29\n",
      "   ---------------- ----------------------- 13.6/32.9 MB 681.9 kB/s eta 0:00:29\n",
      "   ---------------- ----------------------- 13.9/32.9 MB 685.1 kB/s eta 0:00:28\n",
      "   ---------------- ----------------------- 13.9/32.9 MB 685.1 kB/s eta 0:00:28\n",
      "   ----------------- ---------------------- 14.2/32.9 MB 687.8 kB/s eta 0:00:28\n",
      "   ----------------- ---------------------- 14.4/32.9 MB 684.8 kB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 14.4/32.9 MB 684.8 kB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 14.7/32.9 MB 685.1 kB/s eta 0:00:27\n",
      "   ------------------ --------------------- 14.9/32.9 MB 687.3 kB/s eta 0:00:27\n",
      "   ------------------ --------------------- 15.2/32.9 MB 689.5 kB/s eta 0:00:26\n",
      "   ------------------ --------------------- 15.2/32.9 MB 689.5 kB/s eta 0:00:26\n",
      "   ------------------ --------------------- 15.5/32.9 MB 687.3 kB/s eta 0:00:26\n",
      "   ------------------ --------------------- 15.5/32.9 MB 687.3 kB/s eta 0:00:26\n",
      "   ------------------- -------------------- 15.7/32.9 MB 685.3 kB/s eta 0:00:25\n",
      "   ------------------- -------------------- 15.7/32.9 MB 685.3 kB/s eta 0:00:25\n",
      "   ------------------- -------------------- 16.0/32.9 MB 686.5 kB/s eta 0:00:25\n",
      "   ------------------- -------------------- 16.0/32.9 MB 686.5 kB/s eta 0:00:25\n",
      "   ------------------- -------------------- 16.3/32.9 MB 685.5 kB/s eta 0:00:25\n",
      "   -------------------- ------------------- 16.5/32.9 MB 687.5 kB/s eta 0:00:24\n",
      "   -------------------- ------------------- 16.8/32.9 MB 689.1 kB/s eta 0:00:24\n",
      "   -------------------- ------------------- 16.8/32.9 MB 689.1 kB/s eta 0:00:24\n",
      "   -------------------- ------------------- 16.8/32.9 MB 689.1 kB/s eta 0:00:24\n",
      "   -------------------- ------------------- 16.8/32.9 MB 689.1 kB/s eta 0:00:24\n",
      "   -------------------- ------------------- 17.0/32.9 MB 676.6 kB/s eta 0:00:24\n",
      "   --------------------- ------------------ 17.3/32.9 MB 675.7 kB/s eta 0:00:24\n",
      "   --------------------- ------------------ 17.3/32.9 MB 675.7 kB/s eta 0:00:24\n",
      "   --------------------- ------------------ 17.6/32.9 MB 674.9 kB/s eta 0:00:23\n",
      "   --------------------- ------------------ 17.6/32.9 MB 674.9 kB/s eta 0:00:23\n",
      "   --------------------- ------------------ 17.8/32.9 MB 674.3 kB/s eta 0:00:23\n",
      "   ---------------------- ----------------- 18.1/32.9 MB 675.5 kB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 18.1/32.9 MB 675.5 kB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 18.4/32.9 MB 674.6 kB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 18.4/32.9 MB 674.6 kB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 18.6/32.9 MB 668.9 kB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 18.6/32.9 MB 668.9 kB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 18.9/32.9 MB 665.8 kB/s eta 0:00:21\n",
      "   ---------------------- ----------------- 18.9/32.9 MB 665.8 kB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 19.1/32.9 MB 665.5 kB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 19.1/32.9 MB 665.5 kB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 19.4/32.9 MB 661.3 kB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 19.4/32.9 MB 661.3 kB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 19.7/32.9 MB 659.8 kB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 19.7/32.9 MB 659.8 kB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 19.7/32.9 MB 659.8 kB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 19.7/32.9 MB 659.8 kB/s eta 0:00:20\n",
      "   ------------------------ --------------- 19.9/32.9 MB 654.0 kB/s eta 0:00:20\n",
      "   ------------------------ --------------- 19.9/32.9 MB 654.0 kB/s eta 0:00:20\n",
      "   ------------------------ --------------- 20.2/32.9 MB 652.1 kB/s eta 0:00:20\n",
      "   ------------------------ --------------- 20.2/32.9 MB 652.1 kB/s eta 0:00:20\n",
      "   ------------------------ --------------- 20.4/32.9 MB 650.5 kB/s eta 0:00:20\n",
      "   ------------------------ --------------- 20.4/32.9 MB 650.5 kB/s eta 0:00:20\n",
      "   ------------------------- -------------- 20.7/32.9 MB 649.0 kB/s eta 0:00:19\n",
      "   ------------------------- -------------- 20.7/32.9 MB 649.0 kB/s eta 0:00:19\n",
      "   ------------------------- -------------- 20.7/32.9 MB 649.0 kB/s eta 0:00:19\n",
      "   ------------------------- -------------- 21.0/32.9 MB 642.1 kB/s eta 0:00:19\n",
      "   ------------------------- -------------- 21.0/32.9 MB 642.1 kB/s eta 0:00:19\n",
      "   ------------------------- -------------- 21.0/32.9 MB 642.1 kB/s eta 0:00:19\n",
      "   ------------------------- -------------- 21.2/32.9 MB 640.5 kB/s eta 0:00:19\n",
      "   ------------------------- -------------- 21.2/32.9 MB 640.5 kB/s eta 0:00:19\n",
      "   -------------------------- ------------- 21.5/32.9 MB 640.3 kB/s eta 0:00:18\n",
      "   -------------------------- ------------- 21.5/32.9 MB 640.3 kB/s eta 0:00:18\n",
      "   -------------------------- ------------- 21.8/32.9 MB 637.0 kB/s eta 0:00:18\n",
      "   -------------------------- ------------- 21.8/32.9 MB 637.0 kB/s eta 0:00:18\n",
      "   -------------------------- ------------- 22.0/32.9 MB 637.0 kB/s eta 0:00:18\n",
      "   -------------------------- ------------- 22.0/32.9 MB 637.0 kB/s eta 0:00:18\n",
      "   -------------------------- ------------- 22.0/32.9 MB 637.0 kB/s eta 0:00:18\n",
      "   --------------------------- ------------ 22.3/32.9 MB 630.0 kB/s eta 0:00:17\n",
      "   --------------------------- ------------ 22.3/32.9 MB 630.0 kB/s eta 0:00:17\n",
      "   --------------------------- ------------ 22.3/32.9 MB 630.0 kB/s eta 0:00:17\n",
      "   --------------------------- ------------ 22.5/32.9 MB 621.9 kB/s eta 0:00:17\n",
      "   --------------------------- ------------ 22.5/32.9 MB 621.9 kB/s eta 0:00:17\n",
      "   --------------------------- ------------ 22.5/32.9 MB 621.9 kB/s eta 0:00:17\n",
      "   --------------------------- ------------ 22.5/32.9 MB 621.9 kB/s eta 0:00:17\n",
      "   --------------------------- ------------ 22.8/32.9 MB 608.7 kB/s eta 0:00:17\n",
      "   --------------------------- ------------ 22.8/32.9 MB 608.7 kB/s eta 0:00:17\n",
      "   --------------------------- ------------ 22.8/32.9 MB 608.7 kB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 23.1/32.9 MB 596.3 kB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 23.1/32.9 MB 596.3 kB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 23.1/32.9 MB 596.3 kB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 23.3/32.9 MB 586.9 kB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 23.3/32.9 MB 586.9 kB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 23.3/32.9 MB 586.9 kB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 23.6/32.9 MB 570.4 kB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 23.6/32.9 MB 570.4 kB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 23.6/32.9 MB 570.4 kB/s eta 0:00:17\n",
      "   ----------------------------- ---------- 23.9/32.9 MB 561.8 kB/s eta 0:00:17\n",
      "   ----------------------------- ---------- 23.9/32.9 MB 561.8 kB/s eta 0:00:17\n",
      "   ----------------------------- ---------- 24.1/32.9 MB 553.7 kB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 24.1/32.9 MB 553.7 kB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 24.1/32.9 MB 553.7 kB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 24.4/32.9 MB 543.0 kB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 24.4/32.9 MB 543.0 kB/s eta 0:00:16\n",
      "   ------------------------------ --------- 24.6/32.9 MB 539.6 kB/s eta 0:00:16\n",
      "   ------------------------------ --------- 24.6/32.9 MB 539.6 kB/s eta 0:00:16\n",
      "   ------------------------------ --------- 24.6/32.9 MB 539.6 kB/s eta 0:00:16\n",
      "   ------------------------------ --------- 24.9/32.9 MB 535.1 kB/s eta 0:00:15\n",
      "   ------------------------------ --------- 24.9/32.9 MB 535.1 kB/s eta 0:00:15\n",
      "   ------------------------------ --------- 25.2/32.9 MB 529.7 kB/s eta 0:00:15\n",
      "   ------------------------------ --------- 25.2/32.9 MB 529.7 kB/s eta 0:00:15\n",
      "   ------------------------------ --------- 25.4/32.9 MB 525.5 kB/s eta 0:00:15\n",
      "   ------------------------------- -------- 25.7/32.9 MB 525.5 kB/s eta 0:00:14\n",
      "   ------------------------------- -------- 26.2/32.9 MB 527.0 kB/s eta 0:00:13\n",
      "   -------------------------------- ------- 26.5/32.9 MB 528.7 kB/s eta 0:00:13\n",
      "   -------------------------------- ------- 26.5/32.9 MB 528.7 kB/s eta 0:00:13\n",
      "   -------------------------------- ------- 26.7/32.9 MB 530.5 kB/s eta 0:00:12\n",
      "   -------------------------------- ------- 26.7/32.9 MB 530.5 kB/s eta 0:00:12\n",
      "   -------------------------------- ------- 27.0/32.9 MB 528.4 kB/s eta 0:00:12\n",
      "   -------------------------------- ------- 27.0/32.9 MB 528.4 kB/s eta 0:00:12\n",
      "   --------------------------------- ------ 27.3/32.9 MB 529.8 kB/s eta 0:00:11\n",
      "   --------------------------------- ------ 27.5/32.9 MB 530.5 kB/s eta 0:00:11\n",
      "   --------------------------------- ------ 27.5/32.9 MB 530.5 kB/s eta 0:00:11\n",
      "   --------------------------------- ------ 27.8/32.9 MB 533.2 kB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 28.0/32.9 MB 524.5 kB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 28.0/32.9 MB 524.5 kB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 28.3/32.9 MB 526.5 kB/s eta 0:00:09\n",
      "   ---------------------------------- ----- 28.6/32.9 MB 528.8 kB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 28.8/32.9 MB 529.7 kB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 29.1/32.9 MB 529.7 kB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 29.4/32.9 MB 533.5 kB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 29.4/32.9 MB 533.5 kB/s eta 0:00:07\n",
      "   ------------------------------------ --- 29.6/32.9 MB 535.4 kB/s eta 0:00:07\n",
      "   ------------------------------------ --- 29.9/32.9 MB 534.8 kB/s eta 0:00:06\n",
      "   ------------------------------------ --- 30.1/32.9 MB 535.5 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 30.4/32.9 MB 538.0 kB/s eta 0:00:05\n",
      "   ------------------------------------- -- 30.7/32.9 MB 537.4 kB/s eta 0:00:05\n",
      "   ------------------------------------- -- 30.7/32.9 MB 537.4 kB/s eta 0:00:05\n",
      "   ------------------------------------- -- 30.9/32.9 MB 537.5 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 31.2/32.9 MB 536.5 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 31.5/32.9 MB 539.6 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 31.7/32.9 MB 544.0 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 32.0/32.9 MB 546.2 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 32.0/32.9 MB 546.2 kB/s eta 0:00:02\n",
      "   ---------------------------------------  32.2/32.9 MB 546.8 kB/s eta 0:00:02\n",
      "   ---------------------------------------  32.5/32.9 MB 549.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 32.9/32.9 MB 553.4 kB/s  0:00:53\n",
      "Downloading pyttsx3-2.99-py3-none-any.whl (32 kB)\n",
      "Downloading PyAudio-0.2.14-cp313-cp313-win_amd64.whl (173 kB)\n",
      "Downloading comtypes-1.4.12-py3-none-any.whl (253 kB)\n",
      "Downloading pypiwin32-223-py3-none-any.whl (1.7 kB)\n",
      "Downloading standard_aifc-3.13.0-py3-none-any.whl (10 kB)\n",
      "Downloading standard_chunk-3.13.0-py3-none-any.whl (4.9 kB)\n",
      "Installing collected packages: standard-chunk, pyaudio, standard-aifc, pypiwin32, comtypes, SpeechRecognition, pyttsx3\n",
      "\n",
      "   ----- ---------------------------------- 1/7 [pyaudio]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------- ----------------- 4/7 [comtypes]\n",
      "   ---------------------------- ----------- 5/7 [SpeechRecognition]\n",
      "   ---------------------------- ----------- 5/7 [SpeechRecognition]\n",
      "   ---------------------------- ----------- 5/7 [SpeechRecognition]\n",
      "   ---------------------------- ----------- 5/7 [SpeechRecognition]\n",
      "   ---------------------------- ----------- 5/7 [SpeechRecognition]\n",
      "   ---------------------------- ----------- 5/7 [SpeechRecognition]\n",
      "   ---------------------------- ----------- 5/7 [SpeechRecognition]\n",
      "   ---------------------------- ----------- 5/7 [SpeechRecognition]\n",
      "   ---------------------------- ----------- 5/7 [SpeechRecognition]\n",
      "   ---------------------------- ----------- 5/7 [SpeechRecognition]\n",
      "   ---------------------------- ----------- 5/7 [SpeechRecognition]\n",
      "   ---------------------------- ----------- 5/7 [SpeechRecognition]\n",
      "   ---------------------------- ----------- 5/7 [SpeechRecognition]\n",
      "   ---------------------------------- ----- 6/7 [pyttsx3]\n",
      "   ---------------------------------- ----- 6/7 [pyttsx3]\n",
      "   ---------------------------------- ----- 6/7 [pyttsx3]\n",
      "   ---------------------------------- ----- 6/7 [pyttsx3]\n",
      "   ---------------------------------- ----- 6/7 [pyttsx3]\n",
      "   ---------------------------------------- 7/7 [pyttsx3]\n",
      "\n",
      "Successfully installed SpeechRecognition-3.14.3 comtypes-1.4.12 pyaudio-0.2.14 pypiwin32-223 pyttsx3-2.99 standard-aifc-3.13.0 standard-chunk-3.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install SpeechRecognition pyttsx3 pyaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dac73261-7129-4a49-b358-00f2250e78b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Requirement 'PyAudio-0.2.11-cp310-cp310-win_amd64.whl' looks like a filename, but the file does not exist\n",
      "ERROR: PyAudio-0.2.11-cp310-cp310-win_amd64.whl is not a supported wheel on this platform.\n"
     ]
    }
   ],
   "source": [
    "pip install PyAudio-0.2.11-cp310-cp310-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5f107a6-197f-4fae-97ba-cbb4c2005d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤ Ask your question:\n",
      "âŒ Sorry, could not understand speech. Try again.\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "\n",
    "# Initialize\n",
    "recognizer = sr.Recognizer()\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "try:\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"ðŸŽ¤ Ask your question:\")\n",
    "        recognizer.adjust_for_ambient_noise(source, duration=1)  # reduces noise\n",
    "        audio = recognizer.listen(source, timeout=5, phrase_time_limit=10)\n",
    "        \n",
    "        try:\n",
    "            query = recognizer.recognize_google(audio, language=\"en-IN\")  # Indian English\n",
    "            print(\"âœ… You asked:\", query)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"âŒ Sorry, could not understand speech. Try again.\")\n",
    "            query = None\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"âŒ Could not request results; {e}\")\n",
    "            query = None\n",
    "\n",
    "    if query:\n",
    "        # Use your PDF Q&A function here\n",
    "        answer = answer_question(query)\n",
    "        print(\"ðŸ’¡ Answer:\", answer)\n",
    "        engine.say(answer)\n",
    "        engine.runAndWait()\n",
    "\n",
    "except sr.WaitTimeoutError:\n",
    "    print(\"âŒ› Listening timed out. Please speak clearly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afb3013a-7d52-41ca-aea5-c6bd0edb748a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaef0661adcc4b949002b1c9cd344f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Input Type:', layout=Layout(width='50%'), options=('Text', 'Voice'), value='Text')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb125866321456cad19174ad3a149ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Question:', layout=Layout(width='80%'), placeholder='Type your question here...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8a26a8d32440098678ddfe19faf888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Ask AI', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d985c8fc29004ce4afb99a3c255ea10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Radio button to choose input type\n",
    "input_type = widgets.RadioButtons(\n",
    "    options=['Text', 'Voice'],\n",
    "    description='Input Type:',\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "# Textbox for questions\n",
    "question_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type your question here...',\n",
    "    description='Question:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "# Submit button\n",
    "submit_button = widgets.Button(description=\"Ask AI\")\n",
    "\n",
    "# Output area\n",
    "output_area = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "# Handle button click\n",
    "def on_submit(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        if input_type.value == 'Text':\n",
    "            query = question_box.value\n",
    "        else:\n",
    "            # Voice input\n",
    "            import speech_recognition as sr\n",
    "            recognizer = sr.Recognizer()\n",
    "            engine = pyttsx3.init()\n",
    "            try:\n",
    "                with sr.Microphone() as source:\n",
    "                    print(\"Listening...\")\n",
    "                    recognizer.adjust_for_ambient_noise(source, duration=1)\n",
    "                    audio = recognizer.listen(source, timeout=5, phrase_time_limit=10)\n",
    "                    try:\n",
    "                        query = recognizer.recognize_google(audio)\n",
    "                        print(\"You asked:\", query)\n",
    "                    except sr.UnknownValueError:\n",
    "                        print(\"Sorry, I could not understand your speech. Please try again.\")\n",
    "                        return\n",
    "            except sr.WaitTimeoutError:\n",
    "                print(\"Listening timed out. Please speak clearly.\")\n",
    "                return\n",
    "        \n",
    "        if query.strip() == \"\":\n",
    "            print(\"Please enter a question!\")\n",
    "            return\n",
    "        \n",
    "        answer = answer_question(query)\n",
    "        print(\"Answer:\", answer)\n",
    "        if input_type.value == 'Voice':\n",
    "            engine.say(answer)\n",
    "            engine.runAndWait()\n",
    "\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "display(input_type, question_box, submit_button, output_area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22a88988-76d1-4f32-9a11-3cc447e81ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "iface = gr.Interface(fn=answer_question,\n",
    "                     inputs=\"text\",\n",
    "                     outputs=\"text\",\n",
    "                     title=\"PDF Q&A Chatbot\")\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c641ce6-212a-4fed-ae04-335e4bdaceb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\suraj garole\\conda\\lib\\site-packages (5.47.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (4.7.0)\n",
      "Requirement already satisfied: audioop-lts<1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.2.2)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.117.1)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.6.1)\n",
      "Requirement already satisfied: gradio-client==1.13.2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (1.13.2)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.35.1)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (3.11.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (11.1.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (2.10.3)\n",
      "Requirement already satisfied: pydub in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.13.1)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.48.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.19.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.37.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio-client==1.13.2->gradio) (2025.3.2)\n",
      "Requirement already satisfied: websockets<16.0,>=13.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio-client==1.13.2->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\suraj garole\\conda\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\suraj garole\\conda\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\suraj garole\\conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.17.0)\n",
      "Requirement already satisfied: requests in c:\\users\\suraj garole\\conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.27.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\suraj garole\\conda\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ec3c9dd-db29-4819-98c9-27f88c095bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF and building knowledge base...\n",
      "Extracting text with PyPDF2...\n",
      "PyPDF2 found no extractable text. Trying pdfplumber (if installed)...\n",
      "\n",
      "Warning: No selectable text was extracted from the PDF.\n",
      "This may be a scanned PDF (images) and requires OCR (pytesseract + pdf2image) or manual text.\n",
      "You can install pdfplumber (pip install pdfplumber) or use OCR. Exiting build step.\n",
      "\n",
      "Total chunks created: 0\n",
      "No chunks available. The app will run but cannot answer from the PDF until text is available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suraj Garole\\Conda\\Lib\\site-packages\\gradio\\interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to launch Gradio on 127.0.0.1:7860 (try 1/16) ...\n",
      "Port 7860 not available: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\n",
      "Attempting to launch Gradio on 127.0.0.1:7861 (try 2/16) ...\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# gradio_pdf_qa_fixed.py\n",
    "# Single-file, robust PDF â†’ FAISS â†’ Gradio Q&A app.\n",
    "# Usage: python gradio_pdf_qa_fixed.py\n",
    "# (or run cells in Jupyter after removing the __main__ guard)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import gradio as gr\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "pdf_path = r\"C:\\Users\\Suraj Garole\\OneDrive\\Documents\\Python Notes.pdf\"   # <-- fixed path\n",
    "chunk_size = 500\n",
    "overlap = 50\n",
    "embedder_name = \"all-MiniLM-L6-v2\"\n",
    "# ----------------------------\n",
    "\n",
    "def extract_text_pypdf(path):\n",
    "    \"\"\"Try extracting text using PyPDF2.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for i, page in enumerate(reader.pages):\n",
    "                try:\n",
    "                    ptext = page.extract_text()\n",
    "                except Exception:\n",
    "                    ptext = None\n",
    "                if ptext:\n",
    "                    text += ptext + \" \"\n",
    "    except Exception as e:\n",
    "        print(\"PyPDF2 error:\", e)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_pdfplumber(path):\n",
    "    \"\"\"Fallback using pdfplumber if available.\"\"\"\n",
    "    try:\n",
    "        import pdfplumber\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(path) as pdf:\n",
    "            for p in pdf.pages:\n",
    "                ptext = p.extract_text()\n",
    "                if ptext:\n",
    "                    text += ptext + \" \"\n",
    "    except Exception as e:\n",
    "        print(\"pdfplumber error:\", e)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_or_warn(path):\n",
    "    \"\"\"Try multiple extractors and return text (or empty).\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"PDF not found: {path}\")\n",
    "    print(\"Extracting text with PyPDF2...\")\n",
    "    text = extract_text_pypdf(path)\n",
    "    if text:\n",
    "        print(\"Text extracted with PyPDF2 (length:\", len(text), \")\")\n",
    "        return text\n",
    "    print(\"PyPDF2 found no extractable text. Trying pdfplumber (if installed)...\")\n",
    "    text = extract_text_pdfplumber(path)\n",
    "    if text:\n",
    "        print(\"Text extracted with pdfplumber (length:\", len(text), \")\")\n",
    "        return text\n",
    "    # No text found\n",
    "    print(\"\\nWarning: No selectable text was extracted from the PDF.\")\n",
    "    print(\"This may be a scanned PDF (images) and requires OCR (pytesseract + pdf2image) or manual text.\")\n",
    "    print(\"You can install pdfplumber (pip install pdfplumber) or use OCR. Exiting build step.\\n\")\n",
    "    return \"\"\n",
    "\n",
    "def split_text_to_chunks(text, chunk_size=500, overlap=50):\n",
    "    if not text:\n",
    "        return []\n",
    "    words = text.split()\n",
    "    if len(words) == 0:\n",
    "        return []\n",
    "    step = max(1, chunk_size - overlap)\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), step):\n",
    "        chunk = \" \".join(words[i:i+chunk_size]).strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# ---------- Build KB ----------\n",
    "print(\"Loading PDF and building knowledge base...\")\n",
    "try:\n",
    "    all_text = extract_text_or_warn(pdf_path)\n",
    "except FileNotFoundError as e:\n",
    "    print(\"ERROR:\", e)\n",
    "    sys.exit(1)\n",
    "except Exception:\n",
    "    print(\"Unexpected error while loading PDF:\")\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n",
    "\n",
    "chunks = split_text_to_chunks(all_text, chunk_size=chunk_size, overlap=overlap)\n",
    "print(\"Total chunks created:\", len(chunks))\n",
    "\n",
    "# If no chunks, we will still launch Gradio but return helpful message in responses.\n",
    "index = None\n",
    "embedder = None\n",
    "chunk_embeddings = None\n",
    "embedding_dim = 0\n",
    "\n",
    "if len(chunks) == 0:\n",
    "    print(\"No chunks available. The app will run but cannot answer from the PDF until text is available.\")\n",
    "else:\n",
    "    # Create embedder and embeddings\n",
    "    try:\n",
    "        print(\"Loading embedder:\", embedder_name)\n",
    "        embedder = SentenceTransformer(embedder_name)\n",
    "        chunk_embeddings = embedder.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
    "        # If a single chunk, ensure embeddings are 2D\n",
    "        if chunk_embeddings.ndim == 1:\n",
    "            chunk_embeddings = np.expand_dims(chunk_embeddings, 0)\n",
    "        # Safeguard: no-zero dim\n",
    "        if chunk_embeddings.size == 0 or chunk_embeddings.shape[1] == 0:\n",
    "            print(\"Error: embeddings seem empty or zero-dimension. Aborting index build.\")\n",
    "            chunk_embeddings = None\n",
    "        else:\n",
    "            # Ensure float32 contiguous array for FAISS\n",
    "            chunk_embeddings = np.ascontiguousarray(chunk_embeddings.astype('float32'))\n",
    "            embedding_dim = chunk_embeddings.shape[1]\n",
    "            print(\"chunk_embeddings.shape =\", chunk_embeddings.shape)\n",
    "            # Build FAISS index\n",
    "            index = faiss.IndexFlatL2(embedding_dim)\n",
    "            index.add(chunk_embeddings)\n",
    "            print(f\"Built FAISS index. index.d = {index.d}, index.ntotal = {index.ntotal}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error while creating embeddings or building FAISS index:\")\n",
    "        traceback.print_exc()\n",
    "        index = None\n",
    "\n",
    "# ---------- Answer function ----------\n",
    "def answer_question(query, k=3):\n",
    "    \"\"\"Return top-k context chunks or an instructional error message.\"\"\"\n",
    "    if index is None or embedder is None or len(chunks) == 0:\n",
    "        return (\n",
    "            \"Knowledge base is not available. Possible reasons:\\n\"\n",
    "            \"- PDF had no selectable text (scanned images). Try installing pdfplumber or OCR.\\n\"\n",
    "            \"- Embeddings/index failed to build.\\n\\n\"\n",
    "            \"Fix: ensure the PDF contains selectable text, install pdfplumber (pip install pdfplumber) \"\n",
    "            \"or provide a TXT file. Then re-run the KB build.\"\n",
    "        )\n",
    "    try:\n",
    "        q_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "        if q_emb.ndim == 1:\n",
    "            q_emb = q_emb.reshape(1, -1)\n",
    "        q_emb = np.ascontiguousarray(q_emb.astype('float32'))\n",
    "        if q_emb.shape[1] != index.d:\n",
    "            return f\"Embedding dimension mismatch: query vector dim {q_emb.shape[1]} != index dim {index.d}. Rebuild index with the same embedder.\"\n",
    "        # search\n",
    "        D, I = index.search(q_emb, k)\n",
    "        top_chunks = []\n",
    "        for idx in I[0]:\n",
    "            if 0 <= idx < len(chunks):\n",
    "                top_chunks.append(chunks[idx])\n",
    "        if not top_chunks:\n",
    "            return \"No relevant context found.\"\n",
    "        # join with separators to show context\n",
    "        context = \"\\n\\n---\\n\\n\".join(top_chunks)\n",
    "        # For speed/safety we return context. Optionally replace this with an LLM call that uses `context`.\n",
    "        return f\"Top {len(top_chunks)} context chunks (used for answering):\\n\\n{context}\"\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return f\"Error during search/generation: {e}\"\n",
    "\n",
    "# ---------- Launch Gradio with auto-port fallback ----------\n",
    "iface = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a question about the PDF\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"PDF Q&A (FAISS + SentenceTransformers)\",\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "\n",
    "def launch_gradio_with_port_fallback(interface, start_port=7860, max_tries=16, share=False):\n",
    "    port = start_port\n",
    "    for i in range(max_tries):\n",
    "        try:\n",
    "            print(f\"Attempting to launch Gradio on 127.0.0.1:{port} (try {i+1}/{max_tries}) ...\")\n",
    "            interface.launch(server_name=\"127.0.0.1\", server_port=port, share=share, prevent_thread_lock=True)\n",
    "            return\n",
    "        except OSError as e:\n",
    "            print(f\"Port {port} not available: {e}\")\n",
    "            port += 1\n",
    "    raise OSError(f\"Could not find an open port between {start_port} and {port-1}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        launch_gradio_with_port_fallback(iface, start_port=7860, max_tries=16, share=False)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to launch Gradio:\", e)\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "883ec67b-3f81-48cb-a262-46f73a5b828d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\suraj garole\\conda\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\suraj garole\\conda\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pillow in c:\\users\\suraj garole\\conda\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (44.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\suraj garole\\conda\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber pytesseract pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7adf580-0d16-4b38-84f4-5f30359ce27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\suraj garole\\conda\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\suraj garole\\conda\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: gradio in c:\\users\\suraj garole\\conda\\lib\\site-packages (5.47.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from python-docx) (5.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (4.56.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (0.35.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (4.7.0)\n",
      "Requirement already satisfied: audioop-lts<1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.2.2)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.117.1)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.6.1)\n",
      "Requirement already satisfied: gradio-client==1.13.2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (1.13.2)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (3.11.3)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (2.10.3)\n",
      "Requirement already satisfied: pydub in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.13.1)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.48.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.19.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.37.0)\n",
      "Requirement already satisfied: websockets<16.0,>=13.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio-client==1.13.2->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\suraj garole\\conda\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\suraj garole\\conda\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.27.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\suraj garole\\conda\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\suraj garole\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\suraj garole\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx sentence-transformers faiss-cpu gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c8b6a31-30eb-4200-ba4d-dac1b6f59d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\suraj garole\\conda\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pdfplumber) (11.1.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (44.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\suraj garole\\conda\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40cf6340-1aed-46c9-8811-33a3bd7e40bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PDF loaded successfully. Total characters: 21144\n",
      "âœ… Total chunks created: 43\n",
      "ðŸ”„ Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49356524b6204267a7c667e56c6fe5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embeddings generated. Shape: (43, 384)\n",
      "âœ… FAISS index built with 43 vectors.\n",
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import gradio as gr\n",
    "import pdfplumber\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1ï¸âƒ£ Load the PDF text using pdfplumber\n",
    "pdf_path = r\"C:\\Users\\Suraj Garole\\Downloads\\CPO 2022 All Vocab with Hindi meaning by Prashant Sir (1).pdf\"\n",
    "\n",
    "full_text = \"\"\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            full_text += text + \"\\n\"\n",
    "\n",
    "if not full_text.strip():\n",
    "    raise ValueError(\"âŒ No text extracted from the PDF. It might be scanned images.\")\n",
    "\n",
    "print(\"âœ… PDF loaded successfully. Total characters:\", len(full_text))\n",
    "\n",
    "# 2ï¸âƒ£ Split into ~500-character chunks\n",
    "chunk_size = 500\n",
    "chunks = [full_text[i:i + chunk_size] for i in range(0, len(full_text), chunk_size)]\n",
    "print(f\"âœ… Total chunks created: {len(chunks)}\")\n",
    "\n",
    "# 3ï¸âƒ£ Encode with a sentence transformer\n",
    "print(\"ðŸ”„ Generating embeddings...\")\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedder.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
    "print(\"âœ… Embeddings generated. Shape:\", embeddings.shape)\n",
    "\n",
    "# 4ï¸âƒ£ Build a FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "print(f\"âœ… FAISS index built with {index.ntotal} vectors.\")\n",
    "\n",
    "# 5ï¸âƒ£ Define Q&A function\n",
    "def answer_question(query, k=3):\n",
    "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(np.array(q_emb, dtype=\"float32\"), k)\n",
    "    context = \" \".join([chunks[i] for i in I[0]])\n",
    "    return f\"ðŸ’¡ Answer (context-based):\\n\\n{context}\"\n",
    "\n",
    "# 6ï¸âƒ£ Launch Gradio\n",
    "iface = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a question about the PDF content\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"ðŸ“˜ PDF Q&A Chatbot\"\n",
    ")\n",
    "\n",
    "iface.launch(server_name=\"127.0.0.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c12b2af8-001b-41a8-8e6a-1d652cf03265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# gradio_multi_doc_qa.py\n",
    "import os\n",
    "import pickle\n",
    "import traceback\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from docx import Document\n",
    "import PyPDF2\n",
    "\n",
    "# Optional pdfplumber for better PDF extraction\n",
    "try:\n",
    "    import pdfplumber\n",
    "    HAS_PDFPLUMBER = True\n",
    "except Exception:\n",
    "    HAS_PDFPLUMBER = False\n",
    "\n",
    "# Optional OpenAI use\n",
    "try:\n",
    "    import openai\n",
    "    HAS_OPENAI = True\n",
    "except Exception:\n",
    "    HAS_OPENAI = False\n",
    "\n",
    "# ---------- Config ----------\n",
    "EMBEDDER_NAME = \"all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "PERSIST_INDEX_FILE = \"faiss_index.index\"\n",
    "PERSIST_CHUNKS_FILE = \"chunks.pkl\"\n",
    "PERSIST_EMB_FILE = \"chunk_embeddings.npy\"\n",
    "\n",
    "# ---------- Globals ----------\n",
    "embedder = None\n",
    "index = None\n",
    "chunks = []\n",
    "\n",
    "# ---------- Helpers: extract text ----------\n",
    "def extract_text_from_docx(path):\n",
    "    doc = Document(path)\n",
    "    return \"\\n\".join([p.text for p in doc.paragraphs if p.text and p.text.strip()])\n",
    "\n",
    "def extract_text_from_pdf(path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for p in reader.pages:\n",
    "                try:\n",
    "                    page_text = p.extract_text()\n",
    "                except Exception:\n",
    "                    page_text = None\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(\"PyPDF2 read error:\", e)\n",
    "\n",
    "    # fallback to pdfplumber if PyPDF2 gave nothing and pdfplumber is available\n",
    "    if (not text) and HAS_PDFPLUMBER:\n",
    "        try:\n",
    "            with pdfplumber.open(path) as pdf:\n",
    "                for p in pdf.pages:\n",
    "                    ptext = p.extract_text()\n",
    "                    if ptext:\n",
    "                        text += ptext + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(\"pdfplumber error:\", e)\n",
    "\n",
    "    return text\n",
    "\n",
    "def extract_text_from_txt(path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            return f.read()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_text(path):\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".docx\":\n",
    "        return extract_text_from_docx(path)\n",
    "    if ext == \".pdf\":\n",
    "        return extract_text_from_pdf(path)\n",
    "    if ext in (\".txt\", \".md\"):\n",
    "        return extract_text_from_txt(path)\n",
    "    return \"\"  # unsupported file type\n",
    "\n",
    "# ---------- Helpers: split & build ----------\n",
    "def split_text_to_chunks(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    if not text:\n",
    "        return []\n",
    "    words = text.split()\n",
    "    step = max(1, chunk_size - overlap)\n",
    "    chunks_out = []\n",
    "    for i in range(0, len(words), step):\n",
    "        chunk = \" \".join(words[i:i+chunk_size]).strip()\n",
    "        if chunk:\n",
    "            chunks_out.append(chunk)\n",
    "    return chunks_out\n",
    "\n",
    "def build_kb_from_file_paths(file_paths):\n",
    "    \"\"\"\n",
    "    file_paths: list of filesystem paths\n",
    "    returns: status string\n",
    "    \"\"\"\n",
    "    global embedder, index, chunks\n",
    "\n",
    "    # ensure embedder loaded\n",
    "    if embedder is None:\n",
    "        embedder = SentenceTransformer(EMBEDDER_NAME)\n",
    "\n",
    "    texts = []\n",
    "    for p in file_paths:\n",
    "        try:\n",
    "            t = extract_text(p)\n",
    "            if t and t.strip():\n",
    "                texts.append(t)\n",
    "        except Exception as e:\n",
    "            print(\"Error extracting\", p, e)\n",
    "\n",
    "    if not texts:\n",
    "        return \"No selectable text extracted from uploaded files. Try text-based DOCX/TXT or searchable PDFs.\"\n",
    "\n",
    "    # combine and chunk\n",
    "    full_text = \"\\n\\n\".join(texts)\n",
    "    chunks = split_text_to_chunks(full_text)\n",
    "    if not chunks:\n",
    "        return \"Failed to create text chunks from documents.\"\n",
    "\n",
    "    # embeddings\n",
    "    try:\n",
    "        embeddings = embedder.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return f\"Error while encoding embeddings: {e}\"\n",
    "\n",
    "    # shape fixes & dtype\n",
    "    if embeddings.ndim == 1:\n",
    "        embeddings = np.expand_dims(embeddings, 0)\n",
    "    embeddings = np.ascontiguousarray(embeddings.astype(\"float32\"))\n",
    "\n",
    "    if embeddings.size == 0 or embeddings.shape[1] == 0:\n",
    "        return \"Embeddings empty or invalid dimension.\"\n",
    "\n",
    "    # build faiss index\n",
    "    embedding_dim = embeddings.shape[1]\n",
    "    idx = faiss.IndexFlatL2(embedding_dim)\n",
    "    idx.add(embeddings)\n",
    "\n",
    "    # persist\n",
    "    try:\n",
    "        faiss.write_index(idx, PERSIST_INDEX_FILE)\n",
    "        with open(PERSIST_CHUNKS_FILE, \"wb\") as f:\n",
    "            pickle.dump(chunks, f)\n",
    "        np.save(PERSIST_EMB_FILE, embeddings)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: could not persist index to disk:\", e)\n",
    "\n",
    "    # set globals\n",
    "    index = idx\n",
    "\n",
    "    return f\"KB built: {len(chunks)} chunks, embedding_dim={embedding_dim}\"\n",
    "\n",
    "# ---------- Load persisted KB if present ----------\n",
    "def try_load_persisted_kb():\n",
    "    global embedder, index, chunks\n",
    "    if embedder is None:\n",
    "        embedder = SentenceTransformer(EMBEDDER_NAME)\n",
    "    if os.path.exists(PERSIST_INDEX_FILE) and os.path.exists(PERSIST_CHUNKS_FILE):\n",
    "        try:\n",
    "            index = faiss.read_index(PERSIST_INDEX_FILE)\n",
    "            with open(PERSIST_CHUNKS_FILE, \"rb\") as f:\n",
    "                chunks = pickle.load(f)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(\"Failed to load persisted KB:\", e)\n",
    "    return False\n",
    "\n",
    "# ---------- Answering ----------\n",
    "def answer_question(query, top_k=3, use_openai=False):\n",
    "    \"\"\"\n",
    "    Returns either the top-k context chunks OR (optionally) uses OpenAI to generate a final answer.\n",
    "    \"\"\"\n",
    "    global index, chunks, embedder\n",
    "\n",
    "    # ensure embedder\n",
    "    if embedder is None:\n",
    "        embedder = SentenceTransformer(EMBEDDER_NAME)\n",
    "\n",
    "    # attempt to load persisted KB if not in memory\n",
    "    if (index is None or not chunks) and not try_load_persisted_kb():\n",
    "        return \"Knowledge base not ready. Upload files and click 'Build KB' first.\"\n",
    "\n",
    "    # embed query\n",
    "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "    if q_emb.ndim == 1:\n",
    "        q_emb = q_emb.reshape(1, -1)\n",
    "    q_emb = np.ascontiguousarray(q_emb.astype(\"float32\"))\n",
    "\n",
    "    # sanity checks\n",
    "    if q_emb.shape[1] != index.d:\n",
    "        return f\"Embedding dimension mismatch: query dim {q_emb.shape[1]} vs index dim {index.d}. Rebuild KB.\"\n",
    "\n",
    "    # search\n",
    "    D, I = index.search(q_emb, min(top_k, index.ntotal))\n",
    "    top_chunks = []\n",
    "    for idx in I[0]:\n",
    "        if 0 <= idx < len(chunks):\n",
    "            top_chunks.append(chunks[idx])\n",
    "    if not top_chunks:\n",
    "        return \"No relevant context found.\"\n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join(top_chunks)\n",
    "\n",
    "    # Optionally use OpenAI to produce a final answer (requires OPENAI_API_KEY and openai installed)\n",
    "    if use_openai:\n",
    "        if not HAS_OPENAI:\n",
    "            return \"OpenAI package not installed. Install with `pip install openai` to use generation.\"\n",
    "        key = os.getenv(\"OPENAI_API_KEY\") or \"\"\n",
    "        if not key:\n",
    "            return \"OPENAI_API_KEY env var not set. Set it to use OpenAI generation.\"\n",
    "        try:\n",
    "            openai.api_key = key\n",
    "            prompt = f\"Use the provided context to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "            resp = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "                max_tokens=300,\n",
    "                temperature=0.2\n",
    "            )\n",
    "            return resp.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            return f\"OpenAI error: {e}\"\n",
    "\n",
    "    # default: return the retrieved context so user can read the evidence\n",
    "    return f\"Top {len(top_chunks)} context chunks (evidence):\\n\\n{context}\"\n",
    "\n",
    "# ---------- Gradio callbacks ----------\n",
    "def on_build_kb(gr_files):\n",
    "    \"\"\"\n",
    "    gr_files is a list of uploaded file objects (each has .name attribute pointing to temp file path).\n",
    "    \"\"\"\n",
    "    if not gr_files:\n",
    "        return \"No files provided. Upload DOCX, PDF, or TXT files.\"\n",
    "    file_paths = []\n",
    "    for f in gr_files:\n",
    "        # f is a tempfile object; get its path\n",
    "        path = getattr(f, \"name\", None) or f\n",
    "        file_paths.append(path)\n",
    "    return build_kb_from_file_paths(file_paths)\n",
    "\n",
    "def on_ask(query, top_k, use_openai):\n",
    "    if not query or not query.strip():\n",
    "        return \"Please enter a question.\"\n",
    "    return answer_question(query.strip(), top_k=top_k, use_openai=use_openai)\n",
    "\n",
    "# ---------- Gradio UI ----------\n",
    "with gr.Blocks(title=\"Multi-file Q&A (DOCX/PDF/TXT)\") as demo:\n",
    "    gr.Markdown(\"## Upload documents (DOCX / PDF / TXT) and click **Build KB**\")\n",
    "    file_input = gr.File(file_count=\"multiple\", label=\"Upload files (DOCX / PDF / TXT)\")\n",
    "    build_btn = gr.Button(\"Build KB\")\n",
    "    build_out = gr.Textbox(label=\"Build status\", interactive=False)\n",
    "\n",
    "    gr.Markdown(\"## Ask questions (uses the knowledge base)\")\n",
    "    query = gr.Textbox(label=\"Question\", placeholder=\"Type your question here...\")\n",
    "    top_k = gr.Slider(minimum=1, maximum=5, step=1, value=3, label=\"Top-k context chunks\")\n",
    "    use_openai_chk = gr.Checkbox(value=False, label=\"Use OpenAI to generate final answer (optional)\")\n",
    "\n",
    "    ask_btn = gr.Button(\"Ask\")\n",
    "    answer_out = gr.Textbox(label=\"Answer / Retrieved context\", interactive=False)\n",
    "\n",
    "    build_btn.click(fn=on_build_kb, inputs=file_input, outputs=build_out)\n",
    "    ask_btn.click(fn=on_ask, inputs=[query, top_k, use_openai_chk], outputs=answer_out)\n",
    "\n",
    "# If run as script, try to auto-load persisted KB\n",
    "try_load_persisted_kb()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a394845e-4c6e-475d-93c0-9cf1f5676c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "qa_with_ocr_and_local_llm.py\n",
    "\n",
    "Features:\n",
    "- Upload DOCX/PDF/TXT files (Gradio)\n",
    "- Extract text (PyPDF2/pdfplumber) + OCR fallback (pytesseract/pdf2image or ocr.space)\n",
    "- Build FAISS index (SentenceTransformer embeddings)\n",
    "- Answer queries; options: 'none' (show contexts), 'openai' (requires OPENAI_API_KEY), 'local' (offline Flan-T5)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import traceback\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from docx import Document\n",
    "import PyPDF2\n",
    "import time\n",
    "\n",
    "# Optional modules (import inside try blocks)\n",
    "try:\n",
    "    import pdfplumber\n",
    "    HAS_PDFPLUMBER = True\n",
    "except Exception:\n",
    "    HAS_PDFPLUMBER = False\n",
    "\n",
    "try:\n",
    "    import pdf2image\n",
    "    HAS_PDF2IMAGE = True\n",
    "except Exception:\n",
    "    HAS_PDF2IMAGE = False\n",
    "\n",
    "try:\n",
    "    import pytesseract\n",
    "    HAS_PYTESSERACT = True\n",
    "except Exception:\n",
    "    HAS_PYTESSERACT = False\n",
    "\n",
    "try:\n",
    "    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "    HAS_TRANSFORMERS = True\n",
    "except Exception:\n",
    "    HAS_TRANSFORMERS = False\n",
    "\n",
    "try:\n",
    "    import openai\n",
    "    HAS_OPENAI = True\n",
    "except Exception:\n",
    "    HAS_OPENAI = False\n",
    "\n",
    "# ----------------- CONFIG (edit these) -----------------\n",
    "EMBEDDER_NAME = \"all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 400          # smaller chunk size often works better for LLM context\n",
    "CHUNK_OVERLAP = 50\n",
    "PERSIST_INDEX_FILE = \"faiss_index.index\"\n",
    "PERSIST_CHUNKS_FILE = \"chunks.pkl\"\n",
    "PERSIST_EMB_FILE = \"chunk_embeddings.npy\"\n",
    "\n",
    "# OCR options: \"tesseract\", \"ocrspace\", or \"none\"\n",
    "OCR_MODE = \"tesseract\"   # set \"ocrspace\" to use ocr.space API or \"none\" to disable OCR fallback\n",
    "# If OCR_MODE == \"ocrspace\" set OCRSPACE_API_KEY to your key (or leave empty to use free limited usage)\n",
    "OCRSPACE_API_KEY = \"\"     # e.g. \"helloworld\" or your key\n",
    "\n",
    "# If using local Tesseract or Poppler, set these if not on PATH\n",
    "TESSERACT_CMD = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"  # set if needed, else leave blank\n",
    "POPPLER_PATH = r\"C:\\Program Files\\poppler-22.04.0\\Library\\bin\"    # set path to poppler bin on Windows (if needed)\n",
    "\n",
    "# Local LLM options (offline generator)\n",
    "LOCAL_MODEL_NAME = \"google/flan-t5-small\"  # \"flan-t5-small\" or \"flan-t5-base\"\n",
    "USE_CUDA_IF_AVAILABLE = False              # set True if you have GPU and torch+cuda installed\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Globals\n",
    "embedder = None\n",
    "index = None\n",
    "chunks = []\n",
    "local_tokenizer = None\n",
    "local_model = None\n",
    "\n",
    "# ---------- text extraction ----------\n",
    "def extract_text_from_docx(path):\n",
    "    doc = Document(path)\n",
    "    return \"\\n\".join([p.text for p in doc.paragraphs if p.text and p.text.strip()])\n",
    "\n",
    "def extract_text_from_pdf(path):\n",
    "    text = \"\"\n",
    "    # try PyPDF2\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for p in reader.pages:\n",
    "                try:\n",
    "                    page_text = p.extract_text()\n",
    "                except Exception:\n",
    "                    page_text = None\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(\"PyPDF2 read error:\", e)\n",
    "\n",
    "    # fallback to pdfplumber if available and no text\n",
    "    if (not text) and HAS_PDFPLUMBER:\n",
    "        try:\n",
    "            with pdfplumber.open(path) as pdf:\n",
    "                for p in pdf.pages:\n",
    "                    ptext = p.extract_text()\n",
    "                    if ptext:\n",
    "                        text += ptext + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(\"pdfplumber error:\", e)\n",
    "\n",
    "    # OCR fallback if still empty and OCR_MODE configured\n",
    "    if (not text) and OCR_MODE != \"none\":\n",
    "        print(\"No selectable text; attempting OCR fallback (OCR_MODE=%s)...\" % OCR_MODE)\n",
    "        if OCR_MODE == \"tesseract\":\n",
    "            if not HAS_PDF2IMAGE or not HAS_PYTESSERACT:\n",
    "                print(\"pdf2image or pytesseract not installed. Install pdf2image & pytesseract to use local OCR.\")\n",
    "            else:\n",
    "                # optionally set tesseract cmd\n",
    "                if TESSERACT_CMD:\n",
    "                    pytesseract.pytesseract.tesseract_cmd = TESSERACT_CMD\n",
    "                # convert pages to images and OCR\n",
    "                try:\n",
    "                    images = pdf2image.convert_from_path(path, dpi=300, poppler_path=POPPLER_PATH if POPPLER_PATH else None)\n",
    "                    for img in images:\n",
    "                        ocr_text = pytesseract.image_to_string(img)\n",
    "                        if ocr_text:\n",
    "                            text += ocr_text + \"\\n\"\n",
    "                except Exception as e:\n",
    "                    print(\"Local OCR error:\", e, \"Make sure poppler and tesseract are installed if using local OCR.\")\n",
    "        elif OCR_MODE == \"ocrspace\":\n",
    "            # Use ocr.space API\n",
    "            try:\n",
    "                import requests, base64\n",
    "                with open(path, \"rb\") as f:\n",
    "                    files = {\"file\": f}\n",
    "                    data = {\"apikey\": OCRSPACE_API_KEY, \"language\": \"eng\"}\n",
    "                    resp = requests.post(\"https://api.ocr.space/parse/image\", files=files, data=data)\n",
    "                    result = resp.json()\n",
    "                    if result.get(\"ParsedResults\"):\n",
    "                        text = \"\\n\".join([pr.get(\"ParsedText\",\"\") for pr in result[\"ParsedResults\"]])\n",
    "            except Exception as e:\n",
    "                print(\"ocr.space API error:\", e)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_txt(path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            return f.read()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_text(path):\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".docx\":\n",
    "        return extract_text_from_docx(path)\n",
    "    if ext == \".pdf\":\n",
    "        return extract_text_from_pdf(path)\n",
    "    if ext in (\".txt\", \".md\"):\n",
    "        return extract_text_from_txt(path)\n",
    "    return \"\"\n",
    "\n",
    "# ---------- chunking & KB build ----------\n",
    "def split_text_to_chunks(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    if not text:\n",
    "        return []\n",
    "    words = text.split()\n",
    "    step = max(1, chunk_size - overlap)\n",
    "    chunks_out = []\n",
    "    for i in range(0, len(words), step):\n",
    "        chunk = \" \".join(words[i:i+chunk_size]).strip()\n",
    "        if chunk:\n",
    "            chunks_out.append(chunk)\n",
    "    return chunks_out\n",
    "\n",
    "def build_kb_from_paths(paths):\n",
    "    global embedder, index, chunks\n",
    "    if embedder is None:\n",
    "        embedder = SentenceTransformer(EMBEDDER_NAME)\n",
    "\n",
    "    texts = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            t = extract_text(p)\n",
    "            if t and t.strip():\n",
    "                texts.append(t)\n",
    "        except Exception as e:\n",
    "            print(\"Error extracting\", p, e)\n",
    "\n",
    "    if not texts:\n",
    "        return \"No selectable text extracted. Provide DOCX/TXT or searchable PDF, or enable OCR.\"\n",
    "\n",
    "    full_text = \"\\n\\n\".join(texts)\n",
    "    chunks = split_text_to_chunks(full_text)\n",
    "    if not chunks:\n",
    "        return \"Failed to create chunks from text.\"\n",
    "\n",
    "    embeddings = embedder.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
    "    if embeddings.ndim == 1:\n",
    "        embeddings = np.expand_dims(embeddings, 0)\n",
    "    embeddings = np.ascontiguousarray(embeddings.astype(\"float32\"))\n",
    "\n",
    "    if embeddings.size == 0 or embeddings.shape[1] == 0:\n",
    "        return \"Embeddings empty or invalid.\"\n",
    "\n",
    "    idx = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    idx.add(embeddings)\n",
    "\n",
    "    # persist\n",
    "    try:\n",
    "        faiss.write_index(idx, PERSIST_INDEX_FILE)\n",
    "        with open(PERSIST_CHUNKS_FILE, \"wb\") as f:\n",
    "            pickle.dump(chunks, f)\n",
    "        np.save(PERSIST_EMB_FILE, embeddings)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: could not persist index:\", e)\n",
    "\n",
    "    index = idx\n",
    "    return f\"KB built: {len(chunks)} chunks (embedding_dim={embeddings.shape[1]}).\"\n",
    "\n",
    "# ---------- try to load persisted KB ----------\n",
    "def try_load_kb():\n",
    "    global embedder, index, chunks\n",
    "    if embedder is None:\n",
    "        embedder = SentenceTransformer(EMBEDDER_NAME)\n",
    "    if os.path.exists(PERSIST_INDEX_FILE) and os.path.exists(PERSIST_CHUNKS_FILE):\n",
    "        try:\n",
    "            index = faiss.read_index(PERSIST_INDEX_FILE)\n",
    "            with open(PERSIST_CHUNKS_FILE, \"rb\") as f:\n",
    "                chunks = pickle.load(f)\n",
    "            print(\"Loaded persisted KB:\", len(chunks), \"chunks.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(\"Failed loading persisted KB:\", e)\n",
    "    return False\n",
    "\n",
    "# ---------- local LLM (Flan-T5) lazy load ----------\n",
    "def load_local_llm():\n",
    "    global local_model, local_tokenizer\n",
    "    if not HAS_TRANSFORMERS:\n",
    "        raise RuntimeError(\"transformers library not installed.\")\n",
    "    if local_model is None or local_tokenizer is None:\n",
    "        print(\"Loading local model:\", LOCAL_MODEL_NAME)\n",
    "        local_tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_NAME)\n",
    "        local_model = AutoModelForSeq2SeqLM.from_pretrained(LOCAL_MODEL_NAME)\n",
    "        # optionally move to cuda\n",
    "        if USE_CUDA_IF_AVAILABLE:\n",
    "            try:\n",
    "                import torch\n",
    "                if torch.cuda.is_available():\n",
    "                    local_model = local_model.to('cuda')\n",
    "            except Exception as e:\n",
    "                print(\"CUDA move error:\", e)\n",
    "\n",
    "def generate_with_local_model(context, question, max_new_tokens=200):\n",
    "    load_local_llm()\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    inputs = local_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    if USE_CUDA_IF_AVAILABLE:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "            out = local_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "            out_text = local_tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "            return out_text\n",
    "    out = local_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    out_text = local_tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    return out_text\n",
    "\n",
    "# ---------- answering ----------\n",
    "def answer_question(query, top_k=3, generation_mode=\"none\"):\n",
    "    \"\"\"\n",
    "    generation_mode: \"none\" (just return contexts), \"openai\", \"local\"\n",
    "    \"\"\"\n",
    "    global embedder, index, chunks\n",
    "    if embedder is None:\n",
    "        embedder = SentenceTransformer(EMBEDDER_NAME)\n",
    "\n",
    "    # try loading existing KB if not in memory\n",
    "    if (index is None or not chunks) and not try_load_kb():\n",
    "        return \"KB not ready. Upload files and click 'Build KB' first.\"\n",
    "\n",
    "    # embed query\n",
    "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "    if q_emb.ndim == 1:\n",
    "        q_emb = q_emb.reshape(1, -1)\n",
    "    q_emb = np.ascontiguousarray(q_emb.astype(\"float32\"))\n",
    "\n",
    "    if q_emb.shape[1] != index.d:\n",
    "        return f\"Embedding dimension mismatch: query dim {q_emb.shape[1]} vs index dim {index.d}. Rebuild KB.\"\n",
    "\n",
    "    D, I = index.search(q_emb, min(top_k, index.ntotal))\n",
    "    top_chunks = []\n",
    "    for idx in I[0]:\n",
    "        if 0 <= idx < len(chunks):\n",
    "            top_chunks.append(chunks[idx])\n",
    "\n",
    "    if not top_chunks:\n",
    "        return \"No relevant context found.\"\n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join(top_chunks)\n",
    "\n",
    "    # generation modes\n",
    "    if generation_mode == \"none\":\n",
    "        return f\"Top {len(top_chunks)} context chunks:\\n\\n{context}\"\n",
    "    elif generation_mode == \"openai\":\n",
    "        if not HAS_OPENAI:\n",
    "            return \"OpenAI not installed. Install openai package to use this mode.\"\n",
    "        key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "        if not key:\n",
    "            return \"OPENAI_API_KEY not set (environment variable). Set it to use OpenAI.\"\n",
    "        try:\n",
    "            openai.api_key = key\n",
    "            prompt = f\"Answer the question using the context below.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "            resp = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=300,\n",
    "                temperature=0.2\n",
    "            )\n",
    "            return resp.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            return f\"OpenAI error: {e}\"\n",
    "    elif generation_mode == \"local\":\n",
    "        if not HAS_TRANSFORMERS:\n",
    "            return \"Transformers not installed; cannot run local generation.\"\n",
    "        try:\n",
    "            ans = generate_with_local_model(context, query, max_new_tokens=200)\n",
    "            return ans\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            return f\"Local model error: {e}\"\n",
    "    else:\n",
    "        return \"Unknown generation_mode. Use none/openai/local.\"\n",
    "\n",
    "# ---------- Gradio callbacks ----------\n",
    "def on_build_kb(uploaded_files):\n",
    "    if not uploaded_files:\n",
    "        return \"No files uploaded.\"\n",
    "    file_paths = []\n",
    "    for f in uploaded_files:\n",
    "        # f is a temporary file object in Gradio, path is f.name\n",
    "        p = getattr(f, \"name\", None) or f\n",
    "        file_paths.append(p)\n",
    "    return build_kb_from_paths(file_paths)\n",
    "\n",
    "def on_ask(question, top_k, gen_mode):\n",
    "    if not question or not question.strip():\n",
    "        return \"Please enter a question.\"\n",
    "    return answer_question(question.strip(), top_k=top_k, generation_mode=gen_mode)\n",
    "\n",
    "# ---------- Build Gradio UI ----------\n",
    "with gr.Blocks(title=\"Multi-file Q&A with OCR + Local LLM\") as demo:\n",
    "    gr.Markdown(\"## Upload DOCX / PDF / TXT (PDFs may be scanned) and click **Build KB**\")\n",
    "    file_input = gr.File(file_count=\"multiple\", label=\"Upload files\")\n",
    "    build_btn = gr.Button(\"Build KB\")\n",
    "    build_status = gr.Textbox(label=\"Build status\", interactive=False)\n",
    "\n",
    "    gr.Markdown(\"## Ask questions\")\n",
    "    question = gr.Textbox(label=\"Question\")\n",
    "    top_k = gr.Slider(minimum=1, maximum=5, step=1, value=3, label=\"Top-k context chunks\")\n",
    "    gen_mode = gr.Radio(choices=[\"none\", \"openai\", \"local\"], value=\"none\", label=\"Generation mode\")\n",
    "    ask_btn = gr.Button(\"Ask\")\n",
    "    answer_box = gr.Textbox(label=\"Answer / Context\", interactive=False)\n",
    "\n",
    "    build_btn.click(on_build_kb, inputs=file_input, outputs=build_status)\n",
    "    ask_btn.click(on_ask, inputs=[question, top_k, gen_mode], outputs=answer_box)\n",
    "\n",
    "# try to preload persisted KB if available\n",
    "try_load_kb()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5327e13-b09b-44b1-bb79-145a366481fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdf2image in c:\\users\\suraj garole\\conda\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\suraj garole\\conda\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pillow in c:\\users\\suraj garole\\conda\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pytesseract) (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdf2image pytesseract pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3345f6d8-4ccb-4abe-878e-cdc111df5204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\suraj garole\\conda\\lib\\site-packages (5.47.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\suraj garole\\conda\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\suraj garole\\conda\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: python-docx in c:\\users\\suraj garole\\conda\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\suraj garole\\conda\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (4.7.0)\n",
      "Requirement already satisfied: audioop-lts<1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.2.2)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.117.1)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.6.1)\n",
      "Requirement already satisfied: gradio-client==1.13.2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (1.13.2)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.35.1)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (3.11.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (11.1.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (2.10.3)\n",
      "Requirement already satisfied: pydub in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.13.1)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.48.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.19.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio) (0.37.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio-client==1.13.2->gradio) (2025.3.2)\n",
      "Requirement already satisfied: websockets<16.0,>=13.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from gradio-client==1.13.2->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\suraj garole\\conda\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\suraj garole\\conda\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\suraj garole\\conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.17.0)\n",
      "Requirement already satisfied: requests in c:\\users\\suraj garole\\conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.27.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (4.56.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from python-docx) (5.3.0)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (44.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\suraj garole\\conda\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\suraj garole\\conda\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\suraj garole\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\suraj garole\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.3.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "âœ… File loaded. Total characters: 21144\n",
      "âœ… Total chunks created: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55336a0c0284f89a29880348e30d35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embeddings generated.\n",
      "âœ… FAISS index built with 11 vectors.\n",
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================\n",
    "# Step 0: Install required libraries\n",
    "# ===============================\n",
    "!pip install gradio sentence-transformers faiss-cpu python-docx PyPDF2 pdfplumber\n",
    "\n",
    "# ===============================\n",
    "# Step 1: Imports\n",
    "# ===============================\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import gradio as gr\n",
    "\n",
    "# ===============================\n",
    "# Step 2: Load DOCX or PDF text\n",
    "# ===============================\n",
    "def load_file_text(file_path):\n",
    "    text = \"\"\n",
    "    if file_path.lower().endswith(\".pdf\"):\n",
    "        # Using pdfplumber for PDFs (no OCR)\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \" \"\n",
    "    elif file_path.lower().endswith(\".docx\"):\n",
    "        doc = Document(file_path)\n",
    "        for para in doc.paragraphs:\n",
    "            text += para.text + \" \"\n",
    "    elif file_path.lower().endswith(\".txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Use PDF, DOCX, or TXT.\")\n",
    "    return text\n",
    "\n",
    "# âœ… Your file path (CPO 2022 Vocabulary PDF)\n",
    "file_path = r\"C:\\Users\\Suraj Garole\\Downloads\\CPO 2022 All Vocab with Hindi meaning by Prashant Sir (1).pdf\"\n",
    "\n",
    "all_text = load_file_text(file_path)\n",
    "print(\"âœ… File loaded. Total characters:\", len(all_text))\n",
    "\n",
    "# ===============================\n",
    "# Step 3: Split text into chunks\n",
    "# ===============================\n",
    "def split_text(text, chunk_size=400, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text(all_text)\n",
    "print(f\"âœ… Total chunks created: {len(chunks)}\")\n",
    "\n",
    "# ===============================\n",
    "# Step 4: Generate embeddings\n",
    "# ===============================\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "chunk_embeddings = embedder.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# Ensure 2D array\n",
    "if len(chunk_embeddings.shape) == 1:\n",
    "    chunk_embeddings = np.expand_dims(chunk_embeddings, axis=0)\n",
    "\n",
    "print(\"âœ… Embeddings generated.\")\n",
    "\n",
    "# ===============================\n",
    "# Step 5: Build FAISS index\n",
    "# ===============================\n",
    "embedding_dim = chunk_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(chunk_embeddings)\n",
    "print(f\"âœ… FAISS index built with {index.ntotal} vectors.\")\n",
    "\n",
    "# ===============================\n",
    "# Step 6: Define Q&A function\n",
    "# ===============================\n",
    "def answer_question(query, k=3):\n",
    "    # Embed query\n",
    "    query_embedding = embedder.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "    # FAISS search\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    # Retrieve top-k chunks\n",
    "    context = \" \".join([chunks[i] for i in I[0]])\n",
    "    return f\"ðŸ’¡ Answer (context-based):\\n\\n{context}\"\n",
    "\n",
    "# ===============================\n",
    "# Step 7: Gradio UI\n",
    "# ===============================\n",
    "iface = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a question about the document\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"ðŸ“˜ PDF Q&A Chatbot (CPO 2022 Vocabulary)\"\n",
    ")\n",
    "\n",
    "iface.launch(server_name=\"127.0.0.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86527123-ffcd-4869-b178-ff5cb042e05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… File loaded. Total characters: 21144\n",
      "âœ… Total chunks created: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6a2c540f1542d69bdf7634867798bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embeddings generated.\n",
      "âœ… FAISS index built with 11 vectors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8ad98dfa63409685f28dd6856f9d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suraj Garole\\Conda\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Suraj Garole\\.cache\\huggingface\\hub\\models--google--flan-t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f7b9a3bf60445c96ade4e6d7226445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896826ab838c432f9ac2f4cc646a6427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed12984af99f4ecba1ec65acc1aaa897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648256e73f8f40dd846e942892c92857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ff2037bd324e548b3411f20d52c6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff547a736ae46dfbe3bc9df06d2bacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================\n",
    "# Step 0: Install required libraries\n",
    "# ===============================\n",
    "# !pip install gradio sentence-transformers faiss-cpu python-docx PyPDF2 pdfplumber transformers\n",
    "\n",
    "# ===============================\n",
    "# Step 1: Imports\n",
    "# ===============================\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import gradio as gr\n",
    "\n",
    "# ===============================\n",
    "# Step 2: Load DOCX or PDF text\n",
    "# ===============================\n",
    "def load_file_text(file_path):\n",
    "    text = \"\"\n",
    "    if file_path.lower().endswith(\".pdf\"):\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \" \"\n",
    "    elif file_path.lower().endswith(\".docx\"):\n",
    "        doc = Document(file_path)\n",
    "        for para in doc.paragraphs:\n",
    "            text += para.text + \" \"\n",
    "    elif file_path.lower().endswith(\".txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Use PDF, DOCX, or TXT.\")\n",
    "    return text\n",
    "\n",
    "# âœ… Example file path (CPO 2022 Vocabulary PDF)\n",
    "file_path = r\"C:\\Users\\Suraj Garole\\Downloads\\CPO 2022 All Vocab with Hindi meaning by Prashant Sir (1).pdf\"\n",
    "\n",
    "all_text = load_file_text(file_path)\n",
    "print(\"âœ… File loaded. Total characters:\", len(all_text))\n",
    "\n",
    "# ===============================\n",
    "# Step 3: Split text into chunks\n",
    "# ===============================\n",
    "def split_text(text, chunk_size=400, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text(all_text)\n",
    "print(f\"âœ… Total chunks created: {len(chunks)}\")\n",
    "\n",
    "# ===============================\n",
    "# Step 4: Generate embeddings\n",
    "# ===============================\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "chunk_embeddings = embedder.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# Ensure 2D array\n",
    "if len(chunk_embeddings.shape) == 1:\n",
    "    chunk_embeddings = np.expand_dims(chunk_embeddings, axis=0)\n",
    "\n",
    "print(\"âœ… Embeddings generated.\")\n",
    "\n",
    "# ===============================\n",
    "# Step 5: Build FAISS index\n",
    "# ===============================\n",
    "embedding_dim = chunk_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(chunk_embeddings)\n",
    "print(f\"âœ… FAISS index built with {index.ntotal} vectors.\")\n",
    "\n",
    "# ===============================\n",
    "# Step 6: Load small LLM (Flan-T5) for answer generation\n",
    "# ===============================\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# ===============================\n",
    "# Step 7: Define Q&A function\n",
    "# ===============================\n",
    "def answer_question(query, k=3):\n",
    "    # Embed query\n",
    "    query_embedding = embedder.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "    # FAISS search\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    # Retrieve top-k chunks as context\n",
    "    context = \" \".join([chunks[i] for i in I[0]])\n",
    "    \n",
    "    # Prepare prompt for Flan-T5\n",
    "    prompt = f\"Answer the question based on the context below.\\n\\nContext: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    # Generate answer\n",
    "    result = generator(prompt, max_length=200, do_sample=False)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "# ===============================\n",
    "# Step 8: Gradio UI\n",
    "# ===============================\n",
    "iface = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a question about the document\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"ðŸ“˜ PDF Q&A Chatbot (Flan-T5 + FAISS)\"\n",
    ")\n",
    "\n",
    "# Launch Gradio on a free port automatically\n",
    "iface.launch(server_name=\"127.0.0.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61b80927-eba0-4ed7-9bc1-4b325e983333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded. Total characters: 21144\n",
      "Total chunks created: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5375d71c10a44f9491e12329fb61cde0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated.\n",
      "FAISS index built with 11 vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================\n",
    "# Step 0: Install required libraries\n",
    "# ===============================\n",
    "!pip install gradio sentence-transformers faiss-cpu python-docx PyPDF2 pdfplumber transformers pyttsx3 SpeechRecognition pyaudio --quiet\n",
    "\n",
    "# ===============================\n",
    "# Step 1: Imports\n",
    "# ===============================\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import gradio as gr\n",
    "import pyttsx3\n",
    "import speech_recognition as sr\n",
    "\n",
    "# ===============================\n",
    "# Step 2: Load DOCX/PDF/TXT\n",
    "# ===============================\n",
    "def load_file_text(file_path):\n",
    "    text = \"\"\n",
    "    if file_path.lower().endswith(\".pdf\"):\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \" \"\n",
    "    elif file_path.lower().endswith(\".docx\"):\n",
    "        doc = Document(file_path)\n",
    "        for para in doc.paragraphs:\n",
    "            text += para.text + \" \"\n",
    "    elif file_path.lower().endswith(\".txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Use PDF, DOCX, or TXT.\")\n",
    "    return text\n",
    "\n",
    "# âœ… Updated PDF path\n",
    "file_path = r\"C:\\Users\\Suraj Garole\\Downloads\\CPO 2022 All Vocab with Hindi meaning by Prashant Sir (1).pdf\"\n",
    "\n",
    "all_text = load_file_text(file_path)\n",
    "print(\"File loaded. Total characters:\", len(all_text))\n",
    "\n",
    "# ===============================\n",
    "# Step 3: Split text into chunks\n",
    "# ===============================\n",
    "def split_text(text, chunk_size=400, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text(all_text)\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "\n",
    "# ===============================\n",
    "# Step 4: Generate embeddings\n",
    "# ===============================\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "chunk_embeddings = embedder.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# Ensure 2D\n",
    "if len(chunk_embeddings.shape) == 1:\n",
    "    chunk_embeddings = np.expand_dims(chunk_embeddings, axis=0)\n",
    "\n",
    "print(\"Embeddings generated.\")\n",
    "\n",
    "# ===============================\n",
    "# Step 5: Build FAISS index\n",
    "# ===============================\n",
    "embedding_dim = chunk_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(chunk_embeddings)\n",
    "print(f\"FAISS index built with {index.ntotal} vectors.\")\n",
    "\n",
    "# ===============================\n",
    "# Step 6: Load Flan-T5 LLM\n",
    "# ===============================\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# ===============================\n",
    "# Step 7: Setup voice engine\n",
    "# ===============================\n",
    "engine = pyttsx3.init()\n",
    "def speak(text):\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# ===============================\n",
    "# Step 8: Recognize voice\n",
    "# ===============================\n",
    "recognizer = sr.Recognizer()\n",
    "def listen_voice():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "        try:\n",
    "            query = recognizer.recognize_google(audio)\n",
    "            print(\"You said:\", query)\n",
    "            return query\n",
    "        except sr.UnknownValueError:\n",
    "            return \"Sorry, I could not understand your speech.\"\n",
    "        except sr.RequestError:\n",
    "            return \"Could not request results; check your internet connection.\"\n",
    "\n",
    "# ===============================\n",
    "# Step 9: Q&A function\n",
    "# ===============================\n",
    "def answer_question(query, k=3):\n",
    "    query = query.strip()\n",
    "    if query == \"\":\n",
    "        return \"Please ask a question.\"\n",
    "    \n",
    "    # Embed query\n",
    "    query_embedding = embedder.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "    \n",
    "    # FAISS search\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    \n",
    "    # Retrieve top-k chunks\n",
    "    context = \" \".join([chunks[i] for i in I[0]])\n",
    "    \n",
    "    # Decide whether context is useful\n",
    "    if len(context.strip()) < 20:\n",
    "        context = \"\"  # No useful document info\n",
    "    \n",
    "    # Prompt for Flan-T5\n",
    "    if context:\n",
    "        prompt = f\"Answer the question based on the context below.\\n\\nContext: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    else:\n",
    "        prompt = f\"Answer the question:\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    # Generate answer\n",
    "    result = generator(prompt, max_length=200, do_sample=False)\n",
    "    answer = result[0]['generated_text']\n",
    "    \n",
    "    # Speak answer\n",
    "    speak(answer)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# ===============================\n",
    "# Step 10: Gradio interface\n",
    "# ===============================\n",
    "def chatbot_interface(input_type, text_input):\n",
    "    if input_type == \"Voice\":\n",
    "        query = listen_voice()\n",
    "    else:\n",
    "        query = text_input\n",
    "    answer = answer_question(query)\n",
    "    return answer\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=chatbot_interface,\n",
    "    inputs=[gr.Radio([\"Text\", \"Voice\"], label=\"Input Type\"), gr.Textbox(lines=2, placeholder=\"Type your question\")],\n",
    "    outputs=\"text\",\n",
    "    title=\"Universal Q&A Chatbot (Voice + Text + PDF Knowledge)\"\n",
    ")\n",
    "\n",
    "# âœ… Let Gradio select a free port automatically\n",
    "iface.launch(server_name=\"127.0.0.1\", server_port=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c3735bd-739a-44b1-851c-ae6770bc79cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import threading\n",
    "import pyttsx3\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from docx import Document\n",
    "import PyPDF2\n",
    "import gradio as gr\n",
    "\n",
    "# Initialize\n",
    "engine = pyttsx3.init()\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# ----------------- TTS in background -----------------\n",
    "def speak(text):\n",
    "    def run():\n",
    "        engine.say(text)\n",
    "        engine.runAndWait()\n",
    "    t = threading.Thread(target=run)\n",
    "    t.start()\n",
    "\n",
    "# ----------------- Read DOCX -----------------\n",
    "def read_docx(path):\n",
    "    doc = Document(path)\n",
    "    text = \"\"\n",
    "    for para in doc.paragraphs:\n",
    "        text += para.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# ----------------- Read PDF (text only) -----------------\n",
    "def read_pdf(path):\n",
    "    text = \"\"\n",
    "    pdf_file = open(path, 'rb')\n",
    "    reader = PyPDF2.PdfReader(pdf_file)\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# ----------------- Read TXT -----------------\n",
    "def read_txt(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "# ----------------- Load knowledge files -----------------\n",
    "knowledge_texts = []\n",
    "file_paths = [\n",
    "    r\"C:\\Users\\Suraj Garole\\Downloads\\CPO 2022 All Vocab with Hindi meaning by Prashant Sir (1).pdf\",\n",
    "    # Add more files here if needed\n",
    "]\n",
    "\n",
    "for path in file_paths:\n",
    "    if path.endswith('.docx'):\n",
    "        knowledge_texts.append(read_docx(path))\n",
    "    elif path.endswith('.pdf'):\n",
    "        knowledge_texts.append(read_pdf(path))\n",
    "    elif path.endswith('.txt'):\n",
    "        knowledge_texts.append(read_txt(path))\n",
    "\n",
    "# ----------------- Build FAISS index -----------------\n",
    "all_text = \" \".join(knowledge_texts)\n",
    "chunks = [all_text[i:i+500] for i in range(0, len(all_text), 500)]\n",
    "embeddings = model.encode(chunks)\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# ----------------- Chatbot logic -----------------\n",
    "def answer_question(query):\n",
    "    query_vec = model.encode([query])\n",
    "    D, I = index.search(query_vec, k=1)\n",
    "    answer = chunks[I[0][0]]\n",
    "    speak(answer)  # runs in background thread\n",
    "    return answer\n",
    "\n",
    "# ----------------- Gradio Interface -----------------\n",
    "def chatbot_interface(input_type, user_input):\n",
    "    if input_type == \"Text\":\n",
    "        query = user_input\n",
    "    else:\n",
    "        query = user_input  # voice input placeholder\n",
    "    return answer_question(query)\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=chatbot_interface,\n",
    "    inputs=[gr.Radio([\"Text\", \"Voice\"], label=\"Input Type\"),\n",
    "            gr.Textbox(lines=2, placeholder=\"Type your question\")],\n",
    "    outputs=\"text\",\n",
    "    title=\"Universal Q&A Chatbot (Text + Voice + PDF Knowledge)\"\n",
    ")\n",
    "\n",
    "# Launch Gradio (automatically picks free port)\n",
    "iface.launch(server_name=\"127.0.0.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba25bab1-1a47-42e9-92e6-79a76bc063ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'knowledge' created. Place your DOCX/PDF/TXT files there.\n",
      "Using free port: 55152\n",
      "* Running on local URL:  http://127.0.0.1:55152\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:55152/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import socket\n",
    "\n",
    "# ---------------- 1) Knowledge Folder ----------------\n",
    "knowledge_folder = \"knowledge\"\n",
    "if not os.path.exists(knowledge_folder):\n",
    "    os.makedirs(knowledge_folder)\n",
    "    print(f\"Folder '{knowledge_folder}' created. Place your DOCX/PDF/TXT files there.\")\n",
    "\n",
    "# ---------------- 2) Dummy chatbot function ----------------\n",
    "def chatbot_interface(input_type, user_input):\n",
    "    return f\"You said ({input_type}): {user_input}\"\n",
    "\n",
    "# ---------------- 3) Find a free port ----------------\n",
    "def find_free_port():\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.bind((\"\", 0))  # Bind to a free port assigned by OS\n",
    "    port = s.getsockname()[1]\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "free_port = find_free_port()\n",
    "print(f\"Using free port: {free_port}\")\n",
    "\n",
    "# ---------------- 4) Launch Gradio ----------------\n",
    "iface = gr.Interface(\n",
    "    fn=chatbot_interface,\n",
    "    inputs=[gr.Radio([\"Text\", \"Voice\"], label=\"Input Type\"),\n",
    "            gr.Textbox(lines=2, placeholder=\"Type your question here...\")],\n",
    "    outputs=gr.Textbox(label=\"Response\"),\n",
    "    title=\"Local Chatbot\",\n",
    "    description=\"Ask anything! Place DOCX/PDF/TXT files in the 'knowledge' folder to provide answers.\"\n",
    ")\n",
    "\n",
    "iface.launch(server_name=\"127.0.0.1\", server_port=free_port, inbrowser=True, share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4728be8c-a530-4217-bc0a-ba502fe0373e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using free port: 62156\n",
      "* Running on local URL:  http://127.0.0.1:62156\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:62156/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import openai\n",
    "import socket\n",
    "\n",
    "# ---------------- 1) Knowledge Folder ----------------\n",
    "knowledge_folder = \"knowledge\"\n",
    "if not os.path.exists(knowledge_folder):\n",
    "    os.makedirs(knowledge_folder)\n",
    "    print(f\"Folder '{knowledge_folder}' created. Place your DOCX/PDF/TXT files there.\")\n",
    "\n",
    "# ---------------- 2) OpenAI API Key ----------------\n",
    "openai.api_key = \"YOUR_OPENAI_API_KEY\"  # Replace with your key\n",
    "\n",
    "# ---------------- 3) GPT Chatbot Function ----------------\n",
    "def chatbot_interface(input_type, user_input):\n",
    "    \"\"\"\n",
    "    Sends user input to GPT and returns the response.\n",
    "    \"\"\"\n",
    "    if input_type == \"Voice\":\n",
    "        return \"Voice input not implemented yet.\"\n",
    "    \n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",  # or gpt-4 if you have access\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": user_input}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        answer = response['choices'][0]['message']['content']\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# ---------------- 4) Find a free port ----------------\n",
    "def find_free_port():\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.bind((\"\", 0))  # OS assigns a free port\n",
    "    port = s.getsockname()[1]\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "free_port = find_free_port()\n",
    "print(f\"Using free port: {free_port}\")\n",
    "\n",
    "# ---------------- 5) Launch Gradio ----------------\n",
    "iface = gr.Interface(\n",
    "    fn=chatbot_interface,\n",
    "    inputs=[gr.Radio([\"Text\", \"Voice\"], label=\"Input Type\"),\n",
    "            gr.Textbox(lines=2, placeholder=\"Type your question here...\")],\n",
    "    outputs=gr.Textbox(label=\"Response\"),\n",
    "    title=\"GPT-like Chatbot\",\n",
    "    description=\"Ask anything! Place DOCX/PDF/TXT files in the 'knowledge' folder for extra info.\"\n",
    ")\n",
    "\n",
    "iface.launch(server_name=\"127.0.0.1\", server_port=free_port, inbrowser=True, share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce9815c7-64e3-4699-8c7d-a53dc56b14e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:55051\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:55051/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import socket\n",
    "\n",
    "# ----------------- 1) Chatbot function -----------------\n",
    "def chatbot_interface(user_input):\n",
    "    # Replace with your actual model/API\n",
    "    response = f\"You said: {user_input}\"\n",
    "    return response\n",
    "\n",
    "# ----------------- 2) Auto-find a free port -----------------\n",
    "def find_free_port():\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.bind((\"\", 0))  # bind to a free port assigned by the OS\n",
    "    port = s.getsockname()[1]\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "free_port = find_free_port()\n",
    "\n",
    "# ----------------- 3) Create Gradio Interface -----------------\n",
    "iface = gr.Interface(\n",
    "    fn=chatbot_interface,\n",
    "    inputs=gr.Textbox(lines=3, placeholder=\"Ask anything...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Auto GPT-like Chatbot\",\n",
    "    description=\"Type your question and get instant responses.\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "# ----------------- 4) Launch safely -----------------\n",
    "iface.launch(\n",
    "    server_name=\"127.0.0.1\",    # localhost\n",
    "    server_port=free_port,      # guaranteed free port\n",
    "    inbrowser=True,             # automatically opens in browser\n",
    "    prevent_thread_lock=True    # avoids WinError 10061\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec4f548a-dccb-4100-91a5-387b8438ba8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0111584e934e929af2bd655f2bd9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suraj Garole\\Conda\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Suraj Garole\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087e0ed6f7ba4bfcb19241300bbf7ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0631b1fc1ae476baab7278b679cb066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe3ab2041df4a55b566ef089f0d8515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f4d8f1ced447ad8e276b78bc269de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511ad538025d43549f1a429831546454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb9a31788fe4e6c8291466ace9438e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:59350\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:59350/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------- 0) Imports -----------------\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import socket\n",
    "\n",
    "# ----------------- 1) Load GPT-style model -----------------\n",
    "# We'll use a local Hugging Face model for offline capability\n",
    "# Replace with your preferred model (e.g., \"gpt2\" or a fine-tuned GPT)\n",
    "chatbot = pipeline(\"text-generation\", model=\"gpt2\", device=-1)  # CPU; device=0 for GPU\n",
    "\n",
    "# ----------------- 2) Function for Q&A -----------------\n",
    "def gpt_chatbot(user_input):\n",
    "    try:\n",
    "        output = chatbot(user_input, max_length=200, do_sample=True, temperature=0.7)\n",
    "        response = output[0][\"generated_text\"]\n",
    "        # Remove repeated user input\n",
    "        response = response.replace(user_input, \"\").strip()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# ----------------- 3) Auto-find a free port -----------------\n",
    "def find_free_port():\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.bind((\"\", 0))\n",
    "    port = s.getsockname()[1]\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "free_port = find_free_port()\n",
    "\n",
    "# ----------------- 4) Create Gradio Interface -----------------\n",
    "iface = gr.Interface(\n",
    "    fn=gpt_chatbot,\n",
    "    inputs=gr.Textbox(lines=3, placeholder=\"Ask me anything...\"),\n",
    "    outputs=gr.Textbox(label=\"GPT Response\"),\n",
    "    title=\"Local GPT Chatbot\",\n",
    "    description=\"A fully offline GPT-style Q&A chatbot.\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "# ----------------- 5) Launch -----------------\n",
    "iface.launch(\n",
    "    server_name=\"127.0.0.1\",\n",
    "    server_port=free_port,\n",
    "    inbrowser=True,\n",
    "    prevent_thread_lock=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "def5853e-199b-4ebf-aa4b-40452e30ff84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:58502\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:58502/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------- 0) Imports -----------------\n",
    "import gradio as gr\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import socket\n",
    "\n",
    "# ----------------- 1) Load GPT model -----------------\n",
    "# Using GPT-2 for demonstration; you can replace with a larger GPT model if you want\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# ----------------- 2) Auto-find a free port -----------------\n",
    "def find_free_port():\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.bind((\"\", 0))\n",
    "    port = s.getsockname()[1]\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "free_port = find_free_port()\n",
    "\n",
    "# ----------------- 3) Streaming Q&A function -----------------\n",
    "def gpt_chatbot_stream(user_input):\n",
    "    input_ids = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=200,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    # Remove the user input from output\n",
    "    response = output_text.replace(user_input, \"\").strip()\n",
    "    \n",
    "    # Yield each word one by one for streaming effect\n",
    "    for word in response.split():\n",
    "        yield \" \".join(response.split()[:response.split().index(word)+1])\n",
    "\n",
    "# ----------------- 4) Create Gradio Interface -----------------\n",
    "iface = gr.Interface(\n",
    "    fn=gpt_chatbot_stream,\n",
    "    inputs=gr.Textbox(lines=3, placeholder=\"Ask me anything...\"),\n",
    "    outputs=gr.Textbox(label=\"GPT Response\"),\n",
    "    title=\"Local Streaming GPT Chatbot\",\n",
    "    description=\"A fully offline GPT-style chatbot with streaming responses.\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "# ----------------- 5) Launch -----------------\n",
    "iface.launch(\n",
    "    server_name=\"127.0.0.1\",\n",
    "    server_port=free_port,\n",
    "    inbrowser=True,\n",
    "    prevent_thread_lock=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4bc91a7-84e1-4f63-b4b0-f6acb01e7569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1dd22992db465c9a73c415a2204e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suraj Garole\\Conda\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Suraj Garole\\.cache\\huggingface\\hub\\models--google--flan-t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0c051a694e43f68299764f08d66654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3159ccef3f9147d0a74be466f93eb150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91510d26e6c64e838a057f64a4aebab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b342d6367c461ea0674cabd4511326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37384306f6545e3a005e624402cab8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/google/flan-t5-large/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3663a5c5331412bba5a6d78dddace7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  15%|#4        | 461M/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b1e41356304f9f8fb869f967ff62b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:62301\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:62301/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------- Imports -----------------\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import socket\n",
    "\n",
    "# ----------------- Load Q&A GPT model -----------------\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# ----------------- Auto-find a free port -----------------\n",
    "def find_free_port():\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.bind((\"\", 0))\n",
    "    port = s.getsockname()[1]\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "free_port = find_free_port()\n",
    "\n",
    "# ----------------- Q&A Function -----------------\n",
    "def qa_chatbot(user_input):\n",
    "    # Prefix instruction for Q&A\n",
    "    input_text = f\"Question: {user_input} Answer:\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=200,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# ----------------- Gradio Interface -----------------\n",
    "iface = gr.Interface(\n",
    "    fn=qa_chatbot,\n",
    "    inputs=gr.Textbox(lines=3, placeholder=\"Ask any question...\"),\n",
    "    outputs=gr.Textbox(label=\"Answer\"),\n",
    "    title=\"Local GPT Q&A Chatbot\",\n",
    "    description=\"Instruction-tuned Q&A bot, answers questions correctly.\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "# ----------------- Launch -----------------\n",
    "iface.launch(\n",
    "    server_name=\"127.0.0.1\",\n",
    "    server_port=free_port,\n",
    "    inbrowser=True,\n",
    "    prevent_thread_lock=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "519156a3-d4e9-4784-a78a-c0baf20b9087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:50967\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:50967/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------- Imports -----------------\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import socket\n",
    "\n",
    "# ----------------- Load instruction-tuned model -----------------\n",
    "model_name = \"google/flan-t5-large\"  # instruction-tuned for Q&A\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Create a HuggingFace pipeline for text generation\n",
    "qa_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# ----------------- Auto-find a free port -----------------\n",
    "def find_free_port():\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.bind((\"\", 0))\n",
    "    port = s.getsockname()[1]\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "free_port = find_free_port()\n",
    "\n",
    "# ----------------- Q&A Function -----------------\n",
    "def qa_stream(user_input):\n",
    "    # Prefix instruction for Q&A\n",
    "    input_text = f\"Question: {user_input} Answer:\"\n",
    "    # Generate text\n",
    "    outputs = qa_pipeline(input_text, max_length=200, do_sample=True, top_p=0.9, temperature=0.7)\n",
    "    answer = outputs[0][\"generated_text\"]\n",
    "    return answer\n",
    "\n",
    "# ----------------- Gradio Interface -----------------\n",
    "iface = gr.Interface(\n",
    "    fn=qa_stream,\n",
    "    inputs=gr.Textbox(lines=3, placeholder=\"Ask any question...\"),\n",
    "    outputs=gr.Textbox(label=\"Answer\"),\n",
    "    title=\"Streaming GPT Q&A Bot\",\n",
    "    description=\"Instruction-tuned local chatbot that answers your questions accurately.\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "# ----------------- Launch -----------------\n",
    "iface.launch(\n",
    "    server_name=\"127.0.0.1\",\n",
    "    server_port=free_port,\n",
    "    inbrowser=True,\n",
    "    prevent_thread_lock=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e72c9d86-021b-4c18-a244-796e0bcff911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:63852\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:63852/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------- Imports -----------------\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "import socket\n",
    "import time\n",
    "\n",
    "# ----------------- Load instruction-tuned model -----------------\n",
    "# Using Flan-T5 large for general-purpose Q&A (GPT-like)\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# ----------------- Auto-find a free port -----------------\n",
    "def find_free_port():\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.bind((\"\", 0))\n",
    "    port = s.getsockname()[1]\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "free_port = find_free_port()\n",
    "\n",
    "# ----------------- Streaming Q&A Function -----------------\n",
    "def gpt_bot(user_input):\n",
    "    # Instruction prompt: acts like GPT\n",
    "    input_text = f\"You are a highly knowledgeable assistant. Answer the question clearly and accurately:\\n{user_input}\\nAnswer:\"\n",
    "\n",
    "    outputs = qa_pipeline(\n",
    "        input_text,\n",
    "        max_length=500,\n",
    "        do_sample=True,        # allows diverse GPT-like responses\n",
    "        temperature=0.7,       # controls creativity\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    answer = outputs[0][\"generated_text\"]\n",
    "\n",
    "    # Stream word by word\n",
    "    words = answer.split()\n",
    "    partial = \"\"\n",
    "    for w in words:\n",
    "        partial += w + \" \"\n",
    "        yield partial\n",
    "        time.sleep(0.02)  # streaming speed\n",
    "\n",
    "# ----------------- Gradio Interface -----------------\n",
    "iface = gr.Interface(\n",
    "    fn=gpt_bot,\n",
    "    inputs=gr.Textbox(lines=3, placeholder=\"Ask anything...\"),\n",
    "    outputs=gr.Textbox(label=\"Answer\"),\n",
    "    title=\"GPT-style Chatbot\",\n",
    "    description=\"A general-purpose GPT-like chatbot. Ask anything â€” science, engineering, history, coding, etc.\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "# ----------------- Launch -----------------\n",
    "iface.launch(\n",
    "    server_name=\"127.0.0.1\",\n",
    "    server_port=free_port,\n",
    "    inbrowser=True,\n",
    "    prevent_thread_lock=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9c7c087-6388-4e15-9219-a8581878884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:58076\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:58076/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Imports -----------------\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "import socket\n",
    "import time\n",
    "\n",
    "# ----------------- Load instruction-tuned model -----------------\n",
    "# Using Flan-T5 large for general-purpose Q&A (GPT-like)\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# ----------------- Auto-find a free port -----------------\n",
    "def find_free_port():\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.bind((\"\", 0))\n",
    "    port = s.getsockname()[1]\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "free_port = find_free_port()\n",
    "\n",
    "# ----------------- Streaming Q&A Function -----------------\n",
    "def gpt_bot(user_input):\n",
    "    # Instruction prompt: acts like GPT\n",
    "    input_text = f\"You are a highly knowledgeable assistant. Answer the question clearly and accurately:\\n{user_input}\\nAnswer:\"\n",
    "\n",
    "    outputs = qa_pipeline(\n",
    "        input_text,\n",
    "        max_length=500,\n",
    "        do_sample=True,        # allows diverse GPT-like responses\n",
    "        temperature=0.7,       # controls creativity\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    answer = outputs[0][\"generated_text\"]\n",
    "\n",
    "    # Stream word by word\n",
    "    words = answer.split()\n",
    "    partial = \"\"\n",
    "    for w in words:\n",
    "        partial += w + \" \"\n",
    "        yield partial\n",
    "        time.sleep(0.02)  # streaming speed\n",
    "\n",
    "# ----------------- Gradio Interface -----------------\n",
    "iface = gr.Interface(\n",
    "    fn=gpt_bot,\n",
    "    inputs=gr.Textbox(lines=3, placeholder=\"Ask anything...\"),\n",
    "    outputs=gr.Textbox(label=\"Answer\"),\n",
    "    title=\"GPT-style Chatbot\",\n",
    "    description=\"A general-purpose GPT-like chatbot. Ask anything â€” science, engineering, history, coding, etc.\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "# ----------------- Launch -----------------\n",
    "iface.launch(\n",
    "    server_name=\"127.0.0.1\",\n",
    "    server_port=free_port,\n",
    "    inbrowser=True,\n",
    "    prevent_thread_lock=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c422e835-e2a8-4d22-95d3-5025f74ab95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:63546\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:63546/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Imports -----------------\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "import socket\n",
    "import time\n",
    "import pyttsx3\n",
    "import speech_recognition as sr\n",
    "import threading\n",
    "\n",
    "# ----------------- Load instruction-tuned model -----------------\n",
    "# Using Flan-T5 large for general-purpose Q&A (GPT-like)\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# ----------------- Initialize TTS engine -----------------\n",
    "engine = pyttsx3.init()\n",
    "def speak(text):\n",
    "    def run():\n",
    "        engine.say(text)\n",
    "        engine.runAndWait()\n",
    "    t = threading.Thread(target=run)\n",
    "    t.start()\n",
    "\n",
    "# ----------------- Initialize Speech Recognizer -----------------\n",
    "recognizer = sr.Recognizer()\n",
    "def listen_voice():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "        try:\n",
    "            query = recognizer.recognize_google(audio)\n",
    "            print(\"You said:\", query)\n",
    "            return query\n",
    "        except sr.UnknownValueError:\n",
    "            return \"Sorry, I could not understand your speech.\"\n",
    "        except sr.RequestError:\n",
    "            return \"Could not request results; check your internet connection.\"\n",
    "\n",
    "# ----------------- Auto-find a free port -----------------\n",
    "def find_free_port():\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.bind((\"\", 0))\n",
    "    port = s.getsockname()[1]\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "free_port = find_free_port()\n",
    "\n",
    "# ----------------- Streaming Q&A Function -----------------\n",
    "def gpt_bot(input_type, user_input):\n",
    "    # Get query\n",
    "    if input_type == \"Voice\":\n",
    "        query = listen_voice()\n",
    "    else:\n",
    "        query = user_input\n",
    "\n",
    "    # Instruction prompt\n",
    "    input_text = f\"You are a highly knowledgeable assistant. Answer the question clearly and accurately:\\n{query}\\nAnswer:\"\n",
    "\n",
    "    outputs = qa_pipeline(\n",
    "        input_text,\n",
    "        max_length=500,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    answer = outputs[0][\"generated_text\"]\n",
    "\n",
    "    # Speak answer in background\n",
    "    speak(answer)\n",
    "\n",
    "    # Stream answer word by word\n",
    "    words = answer.split()\n",
    "    partial = \"\"\n",
    "    for w in words:\n",
    "        partial += w + \" \"\n",
    "        yield partial\n",
    "        time.sleep(0.02)  # streaming speed\n",
    "\n",
    "# ----------------- Gradio Interface -----------------\n",
    "iface = gr.Interface(\n",
    "    fn=gpt_bot,\n",
    "    inputs=[\n",
    "        gr.Radio([\"Text\", \"Voice\"], label=\"Input Type\"),\n",
    "        gr.Textbox(lines=3, placeholder=\"Type your question (ignored if Voice is selected)\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Answer\"),\n",
    "    title=\"GPT-style Chatbot (Voice + Text Input / Voice + Text Output)\",\n",
    "    description=\"Ask any question via text or voice. The answer will be displayed and spoken aloud.\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "# ----------------- Launch -----------------\n",
    "iface.launch(\n",
    "    server_name=\"127.0.0.1\",\n",
    "    server_port=free_port,\n",
    "    inbrowser=True,\n",
    "    prevent_thread_lock=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887d1ae-0224-4cdc-bd9e-60bcbb06ec18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (transformers)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
